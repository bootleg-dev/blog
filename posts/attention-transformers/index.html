<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformer Architecture | ASR Engineering</title>
<meta name="keywords" content="Attention, Transformer, Deep Learning">
<meta name="description" content="Attention mechanism in Transformers.">
<meta name="author" content="">
<link rel="canonical" href="https://adilsarsenov.dev/posts/attention-transformers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.a72801f0f40a8d7f71aa1cafd1c2f2a993a1f26ca1cfd38fdba65d5b9b0f08a0.css" integrity="sha256-pygB8PQKjX9xqhyv0cLyqZOh8myhz9OP26ZdW5sPCKA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://adilsarsenov.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://adilsarsenov.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://adilsarsenov.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://adilsarsenov.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://adilsarsenov.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Transformer Architecture" />
<meta property="og:description" content="Attention mechanism in Transformers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adilsarsenov.dev/posts/attention-transformers/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-06-02T18:34:44&#43;05:00" />
<meta property="article:modified_time" content="2024-06-02T18:34:44&#43;05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformer Architecture"/>
<meta name="twitter:description" content="Attention mechanism in Transformers."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://adilsarsenov.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Transformer Architecture",
      "item": "https://adilsarsenov.dev/posts/attention-transformers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer Architecture",
  "name": "Transformer Architecture",
  "description": "Attention mechanism in Transformers.",
  "keywords": [
    "Attention", "Transformer", "Deep Learning"
  ],
  "articleBody": "Introduction The Transformer architecture, introduced in the seminal paper “Attention Is All You Need” by Vaswani et al. in 2017, has revolutionized the field of natural language processing (NLP) and beyond with its key innovation: the Attention mechanism.\nThe Transformer Architecture Overview The Transformer is a neural network architecture designed to handle sequential data, particularly in tasks like machine translation, text summarization, and language understanding. Unlike its predecessors (RNNs and LSTMs), Transformers process entire sequences simultaneously, allowing for more parallelization and, consequently, faster training on larger datasets.\nTo understand why this is revolutionary, consider how we typically process language. Traditionally, we’d look at words one by one, trying to understand each in the context of what came before. Transformers, however, look at the entire sentence at once, weighting the importance of each word in relation to all the others. This is akin to understanding the meaning of a sentence by considering all words simultaneously rather than sequentially.\nThe architecture consists of two main components:\nEncoder: Processes the input sequence Decoder: Generates the output sequence Both the encoder and decoder are composed of a stack of identical layers, each containing two sub-layers:\nMulti-Head Attention mechanism Position-wise Fully Connected Feed-Forward Network Key Components 1. Input Embedding Before processing, input tokens (which could be words or subwords) are converted into continuous vector representations. This is typically done using learned embeddings. Mathematically, this can be represented as:\n$E = XW_e$\nWhere:\n$E$ is the embedding matrix $X$ is the one-hot encoded input $W_e$ is the learned embedding weight matrix This formula represents the process of mapping discrete tokens to continuous vector spaces. Each row of $W_e$ corresponds to the embedding vector for a specific token in the vocabulary.\nWhy is this important? Words (or tokens) in language don’t inherently have mathematical meaning. By converting them to vectors, we’re representing them in a way that captures semantic relationships. For instance, in a well-trained embedding space, the vectors for “king” and “queen” might be close to each other, reflecting their related meanings.\n2. Positional Encoding One key difference between Transformers and traditional sequence models is that Transformers don’t inherently understand the order of the input sequence. To address this, we add positional encodings to the input embeddings. This injects information about the position of tokens in the sequence.\nThe positional encoding is calculated using sine and cosine functions:\n$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{model}})$ $PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$\nWhere:\n$pos$ is the position in the sequence $i$ is the dimension $d_{model}$ is the embedding dimension Why use sine and cosine functions? These functions have a beautiful property: for any fixed offset k, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. This allows the model to easily learn to attend to relative positions, which is crucial for understanding language structure.\nThis encoding allows the model to distinguish between different positions in the sequence. Without it, the model would treat “The cat sat on the mat” and “The mat sat on the cat” identically, which would be problematic for understanding meaning!\n3. Multi-Head Attention This is the core innovation of the Transformer architecture, explained in the next section. The key idea is that it allows the model to focus on different parts of the input when producing each part of the output, much like how we might focus on different words when translating a sentence.\n4. Feed-Forward Networks Each attention layer is followed by a position-wise feed-forward network. This consists of two linear transformations with a ReLU activation in between:\n$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\nWhere:\n$x$ is the input to the feed-forward network $W_1$, $W_2$ are weight matrices $b_1$, $b_2$ are bias vectors This feed-forward network is applied to each position separately and identically. It allows the model to introduce non-linearity and increase the representational power of the network. The ReLU activation helps in learning complex patterns.\nWhy is this necessary? The attention mechanism is inherently linear. By adding this non-linear feed-forward network, we’re giving the model the ability to approximate more complex functions, which is crucial for learning intricate patterns in language.\n5. Layer Normalization and Residual Connections After each sub-layer (attention and feed-forward), layer normalization is applied. Additionally, residual connections are used around each sub-layer:\n$LayerNorm(x + Sublayer(x))$\nLayer normalization helps stabilize the learning process by normalizing the inputs across the features. The residual connections allow for deeper networks by providing a direct path for gradients to flow backwards, mitigating the vanishing gradient problem.\nThis combination of normalization and residual connections is crucial for training very deep networks. It helps the model learn stably even with many layers, which is key to the power of large language models like GPT-3.\nThe Attention Mechanism Intuition The attention mechanism is the heart of the Transformer architecture. But what exactly is Attention? In essence, it’s a way for the model to focus on different parts of the input when producing each part of the output.\nThink about how you read a complex sentence. You don’t give equal importance to all words; you focus more on some words to understand the overall meaning. That’s essentially what attention does for the model.\nScaled Dot-Product Attention The basic building block of attention in Transformers is called Scaled Dot-Product Attention. It’s defined as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nWhere:\n$Q$: Query matrix $K$: Key matrix $V$: Value matrix $d_k$: Dimension of the keys Let’s break this down step by step:\nCompute the dot product of the query with all keys: $QK^T$. This gives us a measure of how much each key should be attended to for this particular query. Scale the result by $\\sqrt{d_k}$ to counteract the effect of large dot products in high dimensions. Without this scaling, for large values of $d_k$, the dot products get large, pushing the softmax function into regions where it has extremely small gradients. Apply a softmax function to obtain the weights on the values. This converts the scores to probabilities, ensuring they sum to 1. Multiply the values by their corresponding weights from the softmax. This gives us a weighted sum of the values, where the weights determine how much each value contributes to the output. The intuition here is that we’re deciding how much to attend to different parts of the input (represented by the keys and values) based on what we’re looking for (the query).\nMulti-Head Attention Instead of performing a single attention function, the Transformer uses multi-head attention. This allows the model to jointly attend to information from different representation subspaces at different positions.\nMulti-head attention consists of several attention layers running in parallel:\n$MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^O$\nwhere $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\nHere, the $W$ matrices are learned parameters. Each head can learn to attend to different aspects of the input, allowing for a richer representation.\nWhy is this useful?\nDifferent heads can learn to focus on different aspects of the relationship between words. One head might learn to focus on syntactic relationships, while another might focus on semantic relationships. This multi-faceted approach allows the model to capture a more nuanced understanding of the input.\nTypes of Attention in Transformers Encoder Self-Attention: Each position in the encoder attends to all positions in the previous encoder layer. This allows each token to gather information from all other tokens in the input sequence.\nDecoder Self-Attention: Each position in the decoder attends to all previous positions in the decoder. This is made causal (masked) to prevent positions from attending to subsequent positions, which is necessary for autoregressive generation.\nEncoder-Decoder Attention: Each position in the decoder attends to all positions in the encoder. This allows the decoder to focus on relevant parts of the input sequence for each decoding step.\nThese different types of attention allow the model to capture various types of relationships within and between sequences, enabling it to perform complex language tasks.\nMathematical Deep Dive Let’s break down the mathematics of the attention mechanism even further:\nQuery, Key, and Value Calculation: For each attention head $i$:\n$Q_i = XW_i^Q$ $K_i = XW_i^K$ $V_i = XW_i^V$\nWhere $X$ is the input, and $W$ matrices are learned parameters. This linear transformation allows each head to project the input into a different subspace, enabling the model to capture different types of relationships in the data.\nAttention Scores:\n$S_i = \\frac{Q_iK_i^T}{\\sqrt{d_k}}$\nThis computes a similarity score between each query and key. The scaling factor $\\sqrt{d_k}$ prevents the dot products from growing too large in magnitude, which could push the softmax function into regions with very small gradients.\nAttention Weights:\n$A_i = softmax(S_i)$\nThe softmax function normalizes the scores, converting them into a probability distribution. This determines how much each value will contribute to the output. The softmax ensures that the weights sum to 1 for each query.\nOutput of Each Head:\n$H_i = A_iV_i$\nThis weighted sum of the values represents the output of each attention head. It combines the values based on the attention weights, allowing the model to focus on relevant information for each position.\nConcatenation and Final Projection:\n$MultiHead = Concat(H_1, …, H_h)W^O$\nThe outputs of all heads are concatenated and projected to the desired dimension using another learned weight matrix $W^O$. This final step combines the information from all attention heads into a single representation.\nThis mathematical formulation allows the model to dynamically focus on different parts of the input for each part of the output, enabling it to capture complex relationships in the data.\nWhy Transformers Work So Well? Parallelization: Unlike RNNs, Transformers can process entire sequences in parallel, leading to faster training. This is because the self-attention operation can be computed for all positions simultaneously.\nLong-range Dependencies: The attention mechanism allows the model to directly connect distant positions, mitigating the vanishing gradient problem that plagued RNNs. Every output element is connected to every input element, and the weighted sum of these connections is what allows the model to easily learn long-range dependencies.\nFlexible Context: By using attention, the model can dynamically focus on relevant parts of the input, regardless of their position. This is particularly useful in tasks like translation, where the order of words might change between languages.\nRich Representations: Multi-head attention allows the model to capture different types of relationships in the data simultaneously. Each head can specialize in different aspects of the input, providing a more comprehensive representation.\nInterpretability: The attention weights can be visualized to understand which parts of the input the model is focusing on for each output. This provides some level of interpretability, which is often lacking in deep neural networks.\nScalability: The architecture of Transformers scales well with more data and larger model sizes. This has led to the development of increasingly large and powerful models like GPT-3 and BERT.\nThe Transformer architecture and its attention mechanism have become the backbone of many state-of-the-art models in NLP and beyond. Their ability to process sequential data efficiently while capturing complex relationships has led to breakthroughs in various applications. As research continues, we’re seeing Transformers adapted for vision tasks, multi-modal learning, and even scientific applications like protein folding prediction. Understanding the fundamentals of this architecture is crucial for anyone working in modern machine learning and artificial intelligence.\n",
  "wordCount" : "1861",
  "inLanguage": "en",
  "datePublished": "2024-06-02T18:34:44.165668+05:00",
  "dateModified": "2024-06-02T18:34:44.165668+05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adilsarsenov.dev/posts/attention-transformers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ASR Engineering",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adilsarsenov.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top"><script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header sticky-header">
    <nav class="nav">
        <div class="logo">
            <a href="https://adilsarsenov.dev/" accesskey="h" title="ASR Engineering (Alt + H)">ASR Engineering</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://adilsarsenov.dev/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/posts" title="Recent">
                    <span>Recent</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://adilsarsenov.dev/">Home</a>&nbsp;»&nbsp;<a href="https://adilsarsenov.dev/posts/">Posts</a></div>
    <h1 class="post-title">
      Transformer Architecture
    </h1>
    <div class="post-description">
      Attention mechanism in Transformers.
    </div>
    <div class="post-meta">


Date: 06 June, 2024

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">‎ Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#the-transformer-architecture" aria-label="The Transformer Architecture">The Transformer Architecture</a><ul>
                        
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li></ul>
                </li>
                <li>
                    <a href="#key-components" aria-label="Key Components">Key Components</a><ul>
                        
                <li>
                    <a href="#1-input-embedding" aria-label="1. Input Embedding">1. Input Embedding</a></li>
                <li>
                    <a href="#2-positional-encoding" aria-label="2. Positional Encoding">2. Positional Encoding</a></li>
                <li>
                    <a href="#3-multi-head-attention" aria-label="3. Multi-Head Attention">3. Multi-Head Attention</a></li>
                <li>
                    <a href="#4-feed-forward-networks" aria-label="4. Feed-Forward Networks">4. Feed-Forward Networks</a></li>
                <li>
                    <a href="#5-layer-normalization-and-residual-connections" aria-label="5. Layer Normalization and Residual Connections">5. Layer Normalization and Residual Connections</a></li></ul>
                </li>
                <li>
                    <a href="#the-attention-mechanism" aria-label="The Attention Mechanism">The Attention Mechanism</a></li>
                <li>
                    <a href="#intuition" aria-label="Intuition">Intuition</a></li>
                <li>
                    <a href="#scaled-dot-product-attention" aria-label="Scaled Dot-Product Attention">Scaled Dot-Product Attention</a></li>
                <li>
                    <a href="#multi-head-attention" aria-label="Multi-Head Attention">Multi-Head Attention</a></li>
                <li>
                    <a href="#types-of-attention-in-transformers" aria-label="Types of Attention in Transformers">Types of Attention in Transformers</a></li>
                <li>
                    <a href="#mathematical-deep-dive" aria-label="Mathematical Deep Dive">Mathematical Deep Dive</a></li>
                <li>
                    <a href="#why-transformers-work-so-well" aria-label="Why Transformers Work So Well?">Why Transformers Work So Well?</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>The Transformer architecture, introduced in the seminal paper <strong>&ldquo;Attention Is All You Need&rdquo; by Vaswani et al. in 2017</strong>,
has revolutionized the field of natural language processing (NLP) and beyond with its key innovation: the <strong>Attention</strong> mechanism.</p>
<h3 id="the-transformer-architecture">The Transformer Architecture<a hidden class="anchor" aria-hidden="true" href="#the-transformer-architecture">#</a></h3>
<h4 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h4>
<p>The Transformer is a neural network architecture designed to handle <strong>sequential data</strong>, particularly in tasks like machine translation,
text summarization, and language understanding. Unlike its predecessors (RNNs and LSTMs), Transformers process entire <strong>sequences simultaneously</strong>,
allowing for more parallelization and, consequently, faster training on larger datasets.</p>
<p>To understand why this is revolutionary, consider how we typically process language. Traditionally, we&rsquo;d look at words one by one,
trying to understand each in the context of what came before. Transformers, however, <strong>look at the entire sentence at once, weighting the
importance of each word in relation to all the others</strong>. This is akin to understanding the meaning of a sentence by considering all words simultaneously rather than sequentially.</p>
<p>The architecture consists of two main components:</p>
<ol>
<li><strong>Encoder:</strong> Processes the input sequence</li>
<li><strong>Decoder:</strong> Generates the output sequence</li>
</ol>
<p><img loading="lazy" src="/posts/attention-transformers/img1.png" alt="Attention"  />
</p>
<p>Both the encoder and decoder are composed of a stack of identical layers, each containing two sub-layers:</p>
<ol>
<li><strong>Multi-Head Attention mechanism</strong></li>
<li><strong>Position-wise Fully Connected Feed-Forward Network</strong></li>
</ol>
<h3 id="key-components">Key Components<a hidden class="anchor" aria-hidden="true" href="#key-components">#</a></h3>
<h4 id="1-input-embedding">1. Input Embedding<a hidden class="anchor" aria-hidden="true" href="#1-input-embedding">#</a></h4>
<p>Before processing, input tokens (which could be words or subwords) are converted into continuous vector representations.
This is typically done using learned embeddings. Mathematically, this can be represented as:</p>
<p>$E = XW_e$</p>
<p>Where:</p>
<ul>
<li>$E$ is the embedding matrix</li>
<li>$X$ is the one-hot encoded input</li>
<li>$W_e$ is the learned embedding weight matrix</li>
</ul>
<p>This formula represents the process of mapping discrete tokens to continuous vector spaces. Each row of $W_e$ corresponds
to the embedding vector for a specific token in the vocabulary.</p>
<p>Why is this important? Words (or tokens) in language <strong>don&rsquo;t inherently have mathematical meaning</strong>. By converting them to
vectors, we&rsquo;re representing them in a way that captures semantic relationships. For instance, in a well-trained embedding space,
the vectors for <strong>&ldquo;king&rdquo;</strong> and <strong>&ldquo;queen&rdquo;</strong> might be close to each other, reflecting their related meanings.</p>
<h4 id="2-positional-encoding">2. Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#2-positional-encoding">#</a></h4>
<p>One key difference between Transformers and traditional sequence models is that Transformers don&rsquo;t inherently understand
the order of the input sequence. To address this, we add positional encodings to the input embeddings. This injects information about the position of tokens in the sequence.</p>
<p>The positional encoding is calculated using sine and cosine functions:</p>
<p>$PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})$
$PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p>
<p>Where:</p>
<ul>
<li>$pos$ is the position in the sequence</li>
<li>$i$ is the dimension</li>
<li>$d_{model}$ is the embedding dimension</li>
</ul>
<p>Why use sine and cosine functions? These functions have a beautiful property: for any fixed offset k, $PE_{pos+k}$ can
be represented as a linear function of $PE_{pos}$. This allows the model to easily learn to attend to relative positions,
which is crucial for understanding language structure.</p>
<p>This encoding allows the model to distinguish between different positions in the sequence. Without it, the model would treat
<strong>&ldquo;The cat sat on the mat&rdquo;</strong> and <strong>&ldquo;The mat sat on the cat&rdquo;</strong> identically, which would be problematic for understanding meaning!</p>
<h4 id="3-multi-head-attention">3. Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#3-multi-head-attention">#</a></h4>
<p>This is the core innovation of the Transformer architecture, explained in the next section.
The key idea is that <strong>it allows the model to focus on different parts of the input when producing each part of the output</strong>,
much <strong>like how we might focus on different words when translating a sentence</strong>.</p>
<h4 id="4-feed-forward-networks">4. Feed-Forward Networks<a hidden class="anchor" aria-hidden="true" href="#4-feed-forward-networks">#</a></h4>
<p>Each attention layer is followed by a position-wise feed-forward network. This consists of two linear transformations with a ReLU activation in between:</p>
<p>$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$</p>
<p>Where:</p>
<ul>
<li>$x$ is the input to the feed-forward network</li>
<li>$W_1$, $W_2$ are weight matrices</li>
<li>$b_1$, $b_2$ are bias vectors</li>
</ul>
<p>This feed-forward network is applied to each position separately and identically. It allows the model to introduce
non-linearity and increase the representational power of the network. The ReLU activation helps in learning complex patterns.</p>
<p>Why is this necessary? The attention mechanism is inherently linear. By adding this non-linear feed-forward network,
we&rsquo;re giving the model the ability to approximate more complex functions, which is crucial for learning intricate patterns in language.</p>
<h4 id="5-layer-normalization-and-residual-connections">5. Layer Normalization and Residual Connections<a hidden class="anchor" aria-hidden="true" href="#5-layer-normalization-and-residual-connections">#</a></h4>
<p>After each sub-layer (attention and feed-forward), layer normalization is applied. Additionally, residual connections are used around each sub-layer:</p>
<p>$LayerNorm(x + Sublayer(x))$</p>
<p>Layer normalization helps stabilize the learning process by normalizing the inputs across the features. The residual connections allow for deeper networks by providing a direct path for gradients to flow backwards, mitigating the vanishing gradient problem.</p>
<p>This combination of normalization and residual connections is crucial for training very deep networks. It helps the model learn stably even with many layers, which is key to the power of large language models like GPT-3.</p>
<h3 id="the-attention-mechanism">The Attention Mechanism<a hidden class="anchor" aria-hidden="true" href="#the-attention-mechanism">#</a></h3>
<h3 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h3>
<p>The attention mechanism is the heart of the Transformer architecture. But what exactly is <strong>Attention</strong>?
In essence, <strong>it&rsquo;s a way for the model to focus on different parts of the input when producing each part of the output</strong>.</p>
<p>Think about how you read a complex sentence. You don&rsquo;t give equal importance to all words;
<strong>you focus more on some words to understand the overall meaning</strong>.
<strong>That&rsquo;s essentially what attention does for the model</strong>.</p>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention<a hidden class="anchor" aria-hidden="true" href="#scaled-dot-product-attention">#</a></h3>
<p><img loading="lazy" src="/posts/attention-transformers/img2.png" alt="ScaledDotAttention"  />
</p>
<p>The basic building block of attention in Transformers is called <strong>Scaled Dot-Product Attention</strong>. It&rsquo;s defined as:</p>
<p>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p>
<p>Where:</p>
<ul>
<li>$Q$: Query matrix</li>
<li>$K$: Key matrix</li>
<li>$V$: Value matrix</li>
<li>$d_k$: Dimension of the keys</li>
</ul>
<p>Let&rsquo;s break this down step by step:</p>
<ol>
<li>Compute the dot product of the query with all keys: $QK^T$. This gives us a measure of how much each key should be attended to for this particular query.</li>
<li>Scale the result by $\sqrt{d_k}$ to counteract the effect of large dot products in high dimensions. Without this scaling, for large values of $d_k$, the dot products get large, pushing the softmax function into regions where it has extremely small gradients.</li>
<li>Apply a softmax function to obtain the weights on the values. This converts the scores to probabilities, ensuring they sum to 1.</li>
<li>Multiply the values by their corresponding weights from the softmax. This gives us a weighted sum of the values, where the weights determine how much each value contributes to the output.</li>
</ol>
<p>The intuition here is that <strong>we&rsquo;re deciding how much to attend to different parts of the input (represented by the keys and values)
based on what we&rsquo;re looking for (the query)</strong>.</p>
<h3 id="multi-head-attention">Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">#</a></h3>
<p><img loading="lazy" src="/posts/attention-transformers/img3.png" alt="MultiHeadAttention"  />
</p>
<p>Instead of performing a single attention function, the Transformer uses <strong>multi-head attention</strong>.
This allows the model to <strong>jointly attend to information</strong> from different representation subspaces at different positions.</p>
<p>Multi-head attention consists of several attention layers running in parallel:</p>
<p>$MultiHead(Q, K, V) = Concat(head_1, &hellip;, head_h)W^O$</p>
<p>where $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$</p>
<p>Here, the $W$ matrices are learned parameters. Each head can learn to attend to different aspects of the input, allowing for a richer representation.</p>
<p><strong>Why is this useful?</strong></p>
<p><strong>Different heads can learn to focus on different aspects of the relationship between words</strong>.
One head might learn to focus on <strong>syntactic relationships</strong>, while another might focus on <strong>semantic relationships</strong>.
This multi-faceted approach allows the model to capture a more nuanced understanding of the input.</p>
<h3 id="types-of-attention-in-transformers">Types of Attention in Transformers<a hidden class="anchor" aria-hidden="true" href="#types-of-attention-in-transformers">#</a></h3>
<ol>
<li>
<p><strong>Encoder Self-Attention</strong>: Each position in the encoder attends to all positions in the previous encoder layer. This allows each token to gather information from all other tokens in the input sequence.</p>
</li>
<li>
<p><strong>Decoder Self-Attention</strong>: Each position in the decoder attends to all previous positions in the decoder. This is made <strong>causal (masked)</strong> to prevent positions from attending to subsequent positions, which is necessary for autoregressive generation.</p>
</li>
<li>
<p><strong>Encoder-Decoder Attention</strong>: Each position in the decoder attends to all positions in the encoder. This allows the decoder to focus on relevant parts of the input sequence for each decoding step.</p>
</li>
</ol>
<p>These different types of attention allow the model to capture various types of relationships within and between sequences, enabling it to perform complex language tasks.</p>
<h3 id="mathematical-deep-dive">Mathematical Deep Dive<a hidden class="anchor" aria-hidden="true" href="#mathematical-deep-dive">#</a></h3>
<p>Let&rsquo;s break down the mathematics of the attention mechanism even further:</p>
<ol>
<li>
<p><strong>Query, Key, and Value Calculation</strong>:
For each attention head $i$:</p>
<p>$Q_i = XW_i^Q$
$K_i = XW_i^K$
$V_i = XW_i^V$</p>
<p>Where $X$ is the input, and $W$ matrices are learned parameters. This linear transformation allows each head to project the input into a different subspace, enabling the model to capture different types of relationships in the data.</p>
</li>
<li>
<p><strong>Attention Scores</strong>:</p>
<p>$S_i = \frac{Q_iK_i^T}{\sqrt{d_k}}$</p>
<p>This computes a similarity score between each query and key. The scaling factor $\sqrt{d_k}$ prevents the dot products from growing too large in magnitude, which could push the softmax function into regions with very small gradients.</p>
</li>
<li>
<p><strong>Attention Weights</strong>:</p>
<p>$A_i = softmax(S_i)$</p>
<p>The softmax function normalizes the scores, converting them into a probability distribution. This determines how much each value will contribute to the output. The softmax ensures that the weights sum to 1 for each query.</p>
</li>
<li>
<p><strong>Output of Each Head</strong>:</p>
<p>$H_i = A_iV_i$</p>
<p>This weighted sum of the values represents the output of each attention head. It combines the values based on the attention weights, allowing the model to focus on relevant information for each position.</p>
</li>
<li>
<p><strong>Concatenation and Final Projection</strong>:</p>
<p>$MultiHead = Concat(H_1, &hellip;, H_h)W^O$</p>
<p>The outputs of all heads are concatenated and projected to the desired dimension using another learned weight matrix $W^O$. This final step combines the information from all attention heads into a single representation.</p>
</li>
</ol>
<p>This mathematical formulation allows the model to dynamically focus on different parts of the input for each part of the output, enabling it to capture complex relationships in the data.</p>
<h3 id="why-transformers-work-so-well">Why Transformers Work So Well?<a hidden class="anchor" aria-hidden="true" href="#why-transformers-work-so-well">#</a></h3>
<ol>
<li>
<p><strong>Parallelization</strong>: Unlike RNNs, Transformers can process entire sequences in parallel, leading to faster training. This is because the self-attention operation can be computed for all positions simultaneously.</p>
</li>
<li>
<p><strong>Long-range Dependencies</strong>: The attention mechanism allows the model to directly connect distant positions, mitigating the vanishing gradient problem that plagued RNNs. Every output element is connected to every input element, and the weighted sum of these connections is what allows the model to easily learn long-range dependencies.</p>
</li>
<li>
<p><strong>Flexible Context</strong>: By using attention, the model can dynamically focus on relevant parts of the input, regardless of their position. This is particularly useful in tasks like translation, where the order of words might change between languages.</p>
</li>
<li>
<p><strong>Rich Representations</strong>: Multi-head attention allows the model to capture different types of relationships in the data simultaneously. Each head can specialize in different aspects of the input, providing a more comprehensive representation.</p>
</li>
<li>
<p><strong>Interpretability</strong>: The attention weights can be visualized to understand which parts of the input the model is focusing on for each output. This provides some level of interpretability, which is often lacking in deep neural networks.</p>
</li>
<li>
<p><strong>Scalability</strong>: The architecture of Transformers scales well with more data and larger model sizes. This has led to the development of increasingly large and powerful models like GPT-3 and BERT.</p>
</li>
</ol>
<p>The Transformer architecture and its attention mechanism have become the backbone of many state-of-the-art models in NLP and beyond.
Their ability to process sequential data efficiently while capturing complex relationships has led to breakthroughs in various applications.
As research continues, we&rsquo;re seeing Transformers adapted for <strong>vision tasks, multi-modal learning, and even scientific applications like protein folding prediction</strong>.
Understanding the fundamentals of this architecture is crucial for anyone working in modern machine learning and artificial intelligence.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://adilsarsenov.dev/tags/attention/">Attention</a></li>
      <li><a href="https://adilsarsenov.dev/tags/transformer/">Transformer</a></li>
      <li><a href="https://adilsarsenov.dev/tags/deep-learning/">Deep Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2026 <a href="https://adilsarsenov.dev/">ASR Engineering</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
