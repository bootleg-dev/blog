<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Metrics in Machine Learning and Computer Vision | ASR</title>
<meta name="keywords" content="Beginner, Machine Learning, Computer Vision, Metrics, Evaluation">
<meta name="description" content="Understanding essential metrics for evaluating machine learning models, particularly in computer vision.">
<meta name="author" content="">
<link rel="canonical" href="https://adilsarsenov.dev/posts/metrics/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.a72801f0f40a8d7f71aa1cafd1c2f2a993a1f26ca1cfd38fdba65d5b9b0f08a0.css" integrity="sha256-pygB8PQKjX9xqhyv0cLyqZOh8myhz9OP26ZdW5sPCKA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://adilsarsenov.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://adilsarsenov.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://adilsarsenov.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://adilsarsenov.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://adilsarsenov.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Metrics in Machine Learning and Computer Vision" />
<meta property="og:description" content="Understanding essential metrics for evaluating machine learning models, particularly in computer vision." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adilsarsenov.dev/posts/metrics/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-04-22T18:34:44&#43;05:00" />
<meta property="article:modified_time" content="2024-04-22T18:34:44&#43;05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Metrics in Machine Learning and Computer Vision"/>
<meta name="twitter:description" content="Understanding essential metrics for evaluating machine learning models, particularly in computer vision."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://adilsarsenov.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Metrics in Machine Learning and Computer Vision",
      "item": "https://adilsarsenov.dev/posts/metrics/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Metrics in Machine Learning and Computer Vision",
  "name": "Metrics in Machine Learning and Computer Vision",
  "description": "Understanding essential metrics for evaluating machine learning models, particularly in computer vision.",
  "keywords": [
    "Beginner", "Machine Learning", "Computer Vision", "Metrics", "Evaluation"
  ],
  "articleBody": "Introduction Evaluating model performance is as critical as designing and training the models themselves. Metrics provide quantifiable measures to assess the effectiveness and accuracy of models, guiding decisions for improvement and deployment.\nClassification Metrics Accuracy Accuracy is the most straightforward metric, representing the ratio of correctly predicted instances to the total instances.\n$$ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}} $$\nExample Calculation:\nTrue Positives (TP) = 50 True Negatives (TN) = 40 False Positives (FP) = 10 False Negatives (FN) = 0 Total Instances = 100 $$ \\text{Accuracy} = \\frac{50 + 40}{100} = \\frac{90}{100} = 0.90 , \\text{or} , 90\\text{%} $$\nPrecision, Recall, and F1 Score Precision and recall provide deeper insights, especially in imbalanced datasets.\nPrecision: The ratio of true positive predictions to the total predicted positives. $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$\nExample Calculation:\nTP = 30 FP = 10 $$ \\text{Precision} = \\frac{30}{30 + 10} = \\frac{30}{40} = 0.75 , \\text{or} , 75\\text{%} $$\nRecall: The ratio of true positive predictions to all actual positives. $$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\nExample Calculation:\nTP = 30 FN = 20 $$ \\text{Recall} = \\frac{30}{30 + 20} = \\frac{30}{50} = 0.60 , \\text{or} , 60\\text{%} $$\nF1 Score: The harmonic mean of precision and recall, balancing both metrics. $$ \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nExample Calculation:\nPrecision = 0.75 Recall = 0.60 $$ \\text{F1 Score} = 2 \\cdot \\frac{0.75 \\cdot 0.60}{0.75 + 0.60} = 2 \\cdot \\frac{0.45}{1.35} = \\frac{0.90}{1.35} = 0.67 , \\text{or} , 67\\text{%} $$\nSpecificity and Sensitivity Specificity: The ratio of true negative predictions to all actual negatives. It is used to measure the ability of the model to correctly identify negative instances. $$ \\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}} $$\nExample Calculation:\nTN = 70 FP = 10 $$ \\text{Specificity} = \\frac{70}{70 + 10} = \\frac{70}{80} = 0.875 , \\text{or} , 87.5\\text{%} $$\nSensitivity: Also known as recall, it measures the ability of the model to correctly identify positive instances. $$ \\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\nConfusion Matrix A confusion matrix is a table that provides a detailed breakdown of the performance of a classification model. It shows the number of true positive, true negative, false positive, and false negative predictions.\nExample Calculation:\nPredicted Positive Predicted Negative Actual Positive True Positive (TP) = 50 Type 2 Error: False Negative (FN) = 10 Actual Negative Type 1 Error: False Positive (FP) = 5 True Negative (TN) = 35 ROC-AUC The Receiver Operating Characteristic (ROC) curve is a graphical representation of a model’s ability to discriminate between positive and negative classes across different threshold values. The Area Under the Curve (AUC) quantifies this ability into a single scalar value.\nROC Curve: Plots the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings. AUC: The area under the ROC curve, where an AUC of 1 represents a perfect model, and an AUC of 0.5 represents a model with no discrimination capability. Example Calculation: Suppose we have the following TPR and FPR values at different thresholds:\nThreshold TPR (Recall) FPR (1 - Specificity) 0.1 0.95 0.50 0.2 0.90 0.30 0.3 0.85 0.20 0.4 0.80 0.15 0.5 0.75 0.10 Plotting these points on the ROC curve and calculating the area under this curve gives us the AUC.\nLog Loss (Cross-Entropy Loss) Log Loss evaluates the performance of a classification model where the prediction output is a probability value between 0 and 1.\n$$ \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $$\nExample Calculation:\nSuppose we have 3 instances with the following actual and predicted probabilities: Instance 1: $( y_1 = 1 ), ( \\hat{y}_1 = 0.9 )$ Instance 2: $( y_2 = 0 ), ( \\hat{y}_2 = 0.2 )$ Instance 3: $( y_3 = 1 ), ( \\hat{y}_3 = 0.7 )$ $$ \\text{Log Loss} = -\\frac{1}{3} [(1 \\cdot \\log(0.9) + (1 - 1) \\cdot \\log(1 - 0.9)) + (0 \\cdot \\log(0.2) + (1 - 0) \\cdot \\log(1 - 0.2)) + (1 \\cdot \\log(0.7) + (1 - 1) \\cdot \\log(1 - 0.7))] $$\n$$ \\text{Log Loss} = -\\frac{1}{3} [(-0.105) + (-0.223) + (-0.357)] = -\\frac{1}{3} [-0.685] = 0.228 $$\nMatthews Correlation Coefficient (MCC) MCC is a balanced measure that can be used even if the classes are of very different sizes. It considers true and false positives and negatives.\n$$ \\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} $$\nExample Calculation:\nTP = 50, TN = 40, FP = 10, FN = 10 $$ \\text{MCC} = \\frac{(50 \\cdot 40) - (10 \\cdot 10)}{\\sqrt{(50 + 10)(50 + 10)(40 + 10)(40 + 10)}} = \\frac{2000 - 100}{\\sqrt{60 \\cdot 60 \\cdot 50 \\cdot 50}} = \\frac{1900}{150000} = 0.63 $$\nRegression Metrics Mean Absolute Error (MAE) MAE measures the average magnitude of errors in predictions without considering their direction.\n$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\nExample Calculation:\nErrors = [1, 2, 3] $$ \\text{MAE} = \\frac{1 + 2 + 3}{3} = \\frac{6}{3} = 2 $$\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE) MSE squares the errors before averaging, penalizing larger errors more significantly.\n$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\nExample Calculation:\nErrors = [1, 2, 3] $$ \\text{MSE} = \\frac{1^2 + 2^2 + 3^2}{3} = \\frac{1 + 4 + 9}{3} = \\frac{14}{3} \\approx 4.67 $$\nRMSE is the square root of MSE, bringing the units back to the original scale.\n$$ \\text{RMSE} = \\sqrt{\\text{MSE}} $$\nExample Calculation:\nMSE = 4.67 $$ \\text{RMSE} = \\sqrt{4.67} \\approx 2.16 $$\nR-Squared (R²) R-Squared represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\nExample Calculation:\nTotal Sum of Squares (TSS) = 100 Residual Sum of Squares (RSS) = 20 $$ R^2 = 1 - \\frac{20}{100} = 1 - 0.20 = 0.80 , \\text{or} , 80\\text{%} $$\nMean Absolute Percentage Error (MAPE) MAPE measures the average absolute percentage error between predicted and actual values.\n$$ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $$\nExample Calculation:\nActual values = [100, 200, 300] Predicted values = [110, 190, 310] $$ \\text{MAPE} = \\frac{1}{3} \\left( \\left| \\frac{100 - 110}{100} \\right| + \\left| \\frac{200 - 190}{200} \\right| + \\left| \\frac{300 - 310}{300} \\right| \\right) = \\frac{1}{3} \\left(0.10 + 0.05 + 0.033\\right) = \\frac{1}{3} \\left(0.183\\right) \\approx 0.061 , \\text{or} , 6.1\\text{%} $$\nExplained Variance Score Explained variance measures how much of the variance in the target variable is explained by the model.\n$$ \\text{Explained Variance} = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)} $$\nExample Calculation:\nVariance of residuals = 20 Variance of target = 100 $$ \\text{Explained Variance} = 1 - \\frac{20}{100} = 1 - 0.20 = 0.80 , \\text{or} , 80\\text{%} $$\nComputer Vision Metrics Intersection over Union (IoU) IoU is crucial for segmentation and object detection tasks, measuring the overlap between the predicted and ground truth bounding boxes or segments.\n$$ \\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\nExample Calculation:\nArea of Overlap = 30 Area of Union = 50 $$ \\text{IoU} = \\frac{30}{50} = 0.60 , \\text{or} , 60\\text{%} $$\nMean Average Precision (mAP) mAP is commonly used in object detection, summarizing the precision-recall curve across multiple classes.\nCalculate the precision-recall curve for each class. Compute the Average Precision (AP) for each class. Take the mean of AP values across all classes. Example Calculation:\nAP values for three classes = [0.8, 0.7, 0.9] $$ \\text{mAP} = \\frac{0.8 + 0.7 + 0.9}{3} = \\frac{2.4}{3} = 0.80 , \\text{or} , 80\\text{%} $$\nDice Coefficient The Dice Coefficient, similar to IoU, is another metric for segmentation tasks, focusing on the overlap between predicted and ground truth segments.\n$$ \\text{Dice Coefficient} = \\frac{2 \\times \\text{Area of Overlap}}{\\text{Total Area of Predicted} + \\text{Total Area of Ground Truth}} $$\nExample Calculation:\nArea of Overlap = 30 Total Area of Predicted = 40 Total Area of Ground Truth = 50 $$ \\text{Dice Coefficient} = \\frac{2 \\times 30}{40 + 50} = \\frac{60}{90} = 0.67 , \\text{or} , 67\\text{%} $$\nPixel Accuracy Pixel Accuracy measures the proportion of correctly classified pixels in the entire image.\n$$ \\text{Pixel Accuracy} = \\frac{\\text{Number of Correct Pixels}}{\\text{Total Number of Pixels}} $$\nExample Calculation:\nNumber of Correct Pixels = 900 Total Number of Pixels = 1000 $$ \\text{Pixel Accuracy} = \\frac{900}{1000} = 0.90 , \\text{or} , 90\\text{%} $$\nMean IoU (mIoU) mIoU is the mean of the Intersection over Union (IoU) for all classes. It is commonly used for semantic segmentation tasks.\n$$ \\text{mIoU} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\nExample Calculation:\nIoU values for three classes = [0.6, 0.7, 0.8] $$ \\text{mIoU} = \\frac{0.6 + 0.7 + 0.8}{3} = \\frac{2.1}{3} = 0.70 , \\text{or} , 70\\text{%} $$\nStructural Similarity Index (SSIM) SSIM measures the similarity between two images, considering luminance, contrast, and structure.\n$$ \\text{SSIM}(x, y) = \\frac{(2\\mu_x \\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} $$\nExample Calculation: $$Assume ( \\mu_x = 100 ), ( \\mu_y = 105 ), ( \\sigma_x = 15 ), ( \\sigma_y = 20 ), ( \\sigma_{xy} = 18 ), ( C_1 = 6.5 ), ( C_2 = 58 )$$\n$$ \\text{SSIM} = \\frac{(2 \\cdot 100 \\cdot 105 + 6.5)(2 \\cdot 18 + 58)}{(100^2 + 105^2 + 6.5)(15^2 + 20^2 + 58)} $$\n$$ \\text{SSIM} = \\frac{(21000 + 6.5)(36 + 58)}{(10000 + 11025 + 6.5)(225 + 400 + 58)} $$\n$$ \\text{SSIM} = \\frac{21006.5 \\cdot 94}{21031.5 \\cdot 683} \\approx 0.42 $$\n",
  "wordCount" : "1605",
  "inLanguage": "en",
  "datePublished": "2024-04-22T18:34:44.165668+05:00",
  "dateModified": "2024-04-22T18:34:44.165668+05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adilsarsenov.dev/posts/metrics/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ASR",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adilsarsenov.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top"><script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header sticky-header">
    <nav class="nav">
        <div class="logo">
            <a href="https://adilsarsenov.dev/" accesskey="h" title="ASR (Alt + H)">ASR</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://adilsarsenov.dev/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://adilsarsenov.dev/">Home</a>&nbsp;»&nbsp;<a href="https://adilsarsenov.dev/posts/">Posts</a></div>
    <h1 class="post-title">
      Metrics in Machine Learning and Computer Vision
    </h1>
    <div class="post-description">
      Understanding essential metrics for evaluating machine learning models, particularly in computer vision.
    </div>
    <div class="post-meta">


Date: 22 April, 2024

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">‎ Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#classification-metrics" aria-label="Classification Metrics">Classification Metrics</a><ul>
                        
                <li>
                    <a href="#accuracy" aria-label="Accuracy">Accuracy</a></li>
                <li>
                    <a href="#precision-recall-and-f1-score" aria-label="Precision, Recall, and F1 Score">Precision, Recall, and F1 Score</a></li>
                <li>
                    <a href="#specificity-and-sensitivity" aria-label="Specificity and Sensitivity">Specificity and Sensitivity</a></li>
                <li>
                    <a href="#confusion-matrix" aria-label="Confusion Matrix">Confusion Matrix</a></li>
                <li>
                    <a href="#roc-auc" aria-label="ROC-AUC">ROC-AUC</a></li>
                <li>
                    <a href="#log-loss-cross-entropy-loss" aria-label="Log Loss (Cross-Entropy Loss)">Log Loss (Cross-Entropy Loss)</a></li>
                <li>
                    <a href="#matthews-correlation-coefficient-mcc" aria-label="Matthews Correlation Coefficient (MCC)">Matthews Correlation Coefficient (MCC)</a></li></ul>
                </li>
                <li>
                    <a href="#regression-metrics" aria-label="Regression Metrics">Regression Metrics</a><ul>
                        
                <li>
                    <a href="#mean-absolute-error-mae" aria-label="Mean Absolute Error (MAE)">Mean Absolute Error (MAE)</a></li>
                <li>
                    <a href="#mean-squared-error-mse-and-root-mean-squared-error-rmse" aria-label="Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)">Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)</a></li>
                <li>
                    <a href="#r-squared-r" aria-label="R-Squared (R²)">R-Squared (R²)</a></li>
                <li>
                    <a href="#mean-absolute-percentage-error-mape" aria-label="Mean Absolute Percentage Error (MAPE)">Mean Absolute Percentage Error (MAPE)</a></li>
                <li>
                    <a href="#explained-variance-score" aria-label="Explained Variance Score">Explained Variance Score</a></li></ul>
                </li>
                <li>
                    <a href="#computer-vision-metrics" aria-label="Computer Vision Metrics">Computer Vision Metrics</a><ul>
                        
                <li>
                    <a href="#intersection-over-union-iou" aria-label="Intersection over Union (IoU)">Intersection over Union (IoU)</a></li>
                <li>
                    <a href="#mean-average-precision-map" aria-label="Mean Average Precision (mAP)">Mean Average Precision (mAP)</a></li>
                <li>
                    <a href="#dice-coefficient" aria-label="Dice Coefficient">Dice Coefficient</a></li>
                <li>
                    <a href="#pixel-accuracy" aria-label="Pixel Accuracy">Pixel Accuracy</a></li>
                <li>
                    <a href="#mean-iou-miou" aria-label="Mean IoU (mIoU)">Mean IoU (mIoU)</a></li>
                <li>
                    <a href="#structural-similarity-index-ssim" aria-label="Structural Similarity Index (SSIM)">Structural Similarity Index (SSIM)</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>Evaluating model performance is as critical as designing and training the models themselves. Metrics provide quantifiable measures to assess the effectiveness and accuracy of models, guiding decisions for improvement and deployment.</p>
<h3 id="classification-metrics">Classification Metrics<a hidden class="anchor" aria-hidden="true" href="#classification-metrics">#</a></h3>
<h4 id="accuracy">Accuracy<a hidden class="anchor" aria-hidden="true" href="#accuracy">#</a></h4>
<p>Accuracy is the most straightforward metric, representing the <strong>ratio of correctly predicted instances to the total instances</strong>.</p>
<p>$$
\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>True Positives (TP) = 50</li>
<li>True Negatives (TN) = 40</li>
<li>False Positives (FP) = 10</li>
<li>False Negatives (FN) = 0</li>
<li>Total Instances = 100</li>
</ul>
<p>$$
\text{Accuracy} = \frac{50 + 40}{100} = \frac{90}{100} = 0.90 , \text{or} , 90\text{%}
$$</p>
<h4 id="precision-recall-and-f1-score">Precision, Recall, and F1 Score<a hidden class="anchor" aria-hidden="true" href="#precision-recall-and-f1-score">#</a></h4>
<p>Precision and recall provide deeper insights, especially in <strong>imbalanced datasets</strong>.</p>
<ul>
<li><strong>Precision</strong>: The ratio of true positive predictions to the total predicted positives.</li>
</ul>
<p>$$
\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>TP = 30</li>
<li>FP = 10</li>
</ul>
<p>$$
\text{Precision} = \frac{30}{30 + 10} = \frac{30}{40} = 0.75 , \text{or} , 75\text{%}
$$</p>
<ul>
<li><strong>Recall</strong>: The ratio of true positive predictions to all actual positives.</li>
</ul>
<p>$$
\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>TP = 30</li>
<li>FN = 20</li>
</ul>
<p>$$
\text{Recall} = \frac{30}{30 + 20} = \frac{30}{50} = 0.60 , \text{or} , 60\text{%}
$$</p>
<ul>
<li><strong>F1 Score</strong>: The harmonic mean of precision and recall, balancing both metrics.</li>
</ul>
<p>$$
\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Precision = 0.75</li>
<li>Recall = 0.60</li>
</ul>
<p>$$
\text{F1 Score} = 2 \cdot \frac{0.75 \cdot 0.60}{0.75 + 0.60} = 2 \cdot \frac{0.45}{1.35} = \frac{0.90}{1.35} = 0.67 , \text{or} , 67\text{%}
$$</p>
<h4 id="specificity-and-sensitivity">Specificity and Sensitivity<a hidden class="anchor" aria-hidden="true" href="#specificity-and-sensitivity">#</a></h4>
<ul>
<li><strong>Specificity</strong>: The ratio of true negative predictions to all actual negatives. It is used to measure the ability of the model to correctly identify negative instances.</li>
</ul>
<p>$$
\text{Specificity} = \frac{\text{True Negatives}}{\text{True Negatives} + \text{False Positives}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>TN = 70</li>
<li>FP = 10</li>
</ul>
<p>$$
\text{Specificity} = \frac{70}{70 + 10} = \frac{70}{80} = 0.875 , \text{or} , 87.5\text{%}
$$</p>
<ul>
<li><strong>Sensitivity</strong>: Also known as recall, it measures the ability of the model to correctly identify positive instances.</li>
</ul>
<p>$$
\text{Sensitivity} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
$$</p>
<h4 id="confusion-matrix">Confusion Matrix<a hidden class="anchor" aria-hidden="true" href="#confusion-matrix">#</a></h4>
<p>A confusion matrix is a table that provides a detailed breakdown of the performance of a classification model. It shows the number of true positive, true negative, false positive, and false negative predictions.</p>
<p><strong>Example Calculation</strong>:</p>
<table>
<thead>
<tr>
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actual Positive</td>
<td>True Positive (TP) = 50</td>
<td><strong>Type 2 Error</strong>: False Negative (FN) = 10</td>
</tr>
<tr>
<td>Actual Negative</td>
<td><strong>Type 1 Error</strong>: False Positive (FP) = 5</td>
<td>True Negative (TN) = 35</td>
</tr>
</tbody>
</table>
<h4 id="roc-auc">ROC-AUC<a hidden class="anchor" aria-hidden="true" href="#roc-auc">#</a></h4>
<p>The Receiver Operating Characteristic <strong>(ROC) curve</strong> is a graphical representation of a model&rsquo;s ability to discriminate between
<strong>positive and negative classes across different threshold values</strong>. The Area Under the Curve (AUC) quantifies this ability into a single scalar value.</p>
<ul>
<li><strong>ROC Curve</strong>: Plots the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings.</li>
<li><strong>AUC</strong>: The area under the ROC curve, where an AUC of 1 represents a perfect model, and an AUC of 0.5 represents a model with no discrimination capability.</li>
</ul>
<p><strong>Example Calculation</strong>: Suppose we have the following TPR and FPR values at different thresholds:</p>
<table>
<thead>
<tr>
<th>Threshold</th>
<th>TPR (Recall)</th>
<th>FPR (1 - Specificity)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.1</td>
<td>0.95</td>
<td>0.50</td>
</tr>
<tr>
<td>0.2</td>
<td>0.90</td>
<td>0.30</td>
</tr>
<tr>
<td>0.3</td>
<td>0.85</td>
<td>0.20</td>
</tr>
<tr>
<td>0.4</td>
<td>0.80</td>
<td>0.15</td>
</tr>
<tr>
<td>0.5</td>
<td>0.75</td>
<td>0.10</td>
</tr>
</tbody>
</table>
<p>Plotting these points on the ROC curve and calculating the area under this curve gives us the AUC.</p>
<h4 id="log-loss-cross-entropy-loss">Log Loss (Cross-Entropy Loss)<a hidden class="anchor" aria-hidden="true" href="#log-loss-cross-entropy-loss">#</a></h4>
<p>Log Loss evaluates the performance of a classification model where the prediction output is a <strong>probability value between 0 and 1</strong>.</p>
<p>$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Suppose we have 3 instances with the following actual and predicted probabilities:
<ul>
<li>Instance 1: $( y_1 = 1 ), ( \hat{y}_1 = 0.9 )$</li>
<li>Instance 2: $( y_2 = 0 ), ( \hat{y}_2 = 0.2 )$</li>
<li>Instance 3: $( y_3 = 1 ), ( \hat{y}_3 = 0.7 )$</li>
</ul>
</li>
</ul>
<p>$$
\text{Log Loss} = -\frac{1}{3} [(1 \cdot \log(0.9) + (1 - 1) \cdot \log(1 - 0.9)) + (0 \cdot \log(0.2) + (1 - 0) \cdot \log(1 - 0.2)) + (1 \cdot \log(0.7) + (1 - 1) \cdot \log(1 - 0.7))]
$$</p>
<p>$$
\text{Log Loss} = -\frac{1}{3} [(-0.105) + (-0.223) + (-0.357)] = -\frac{1}{3} [-0.685] = 0.228
$$</p>
<h4 id="matthews-correlation-coefficient-mcc">Matthews Correlation Coefficient (MCC)<a hidden class="anchor" aria-hidden="true" href="#matthews-correlation-coefficient-mcc">#</a></h4>
<p>MCC is a balanced measure that can be used even if the classes are of very different sizes. It considers true and false positives and negatives.</p>
<p>$$
\text{MCC} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>TP = 50, TN = 40, FP = 10, FN = 10</li>
</ul>
<p>$$
\text{MCC} = \frac{(50 \cdot 40) - (10 \cdot 10)}{\sqrt{(50 + 10)(50 + 10)(40 + 10)(40 + 10)}} = \frac{2000 - 100}{\sqrt{60 \cdot 60 \cdot 50 \cdot 50}} = \frac{1900}{150000} = 0.63
$$</p>
<h3 id="regression-metrics">Regression Metrics<a hidden class="anchor" aria-hidden="true" href="#regression-metrics">#</a></h3>
<h4 id="mean-absolute-error-mae">Mean Absolute Error (MAE)<a hidden class="anchor" aria-hidden="true" href="#mean-absolute-error-mae">#</a></h4>
<p>MAE measures the average magnitude of errors in predictions without considering their direction.</p>
<p>$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Errors = [1, 2, 3]</li>
</ul>
<p>$$
\text{MAE} = \frac{1 + 2 + 3}{3} = \frac{6}{3} = 2
$$</p>
<h4 id="mean-squared-error-mse-and-root-mean-squared-error-rmse">Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)<a hidden class="anchor" aria-hidden="true" href="#mean-squared-error-mse-and-root-mean-squared-error-rmse">#</a></h4>
<p>MSE squares the errors before averaging, penalizing larger errors more significantly.</p>
<p>$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Errors = [1, 2, 3]</li>
</ul>
<p>$$
\text{MSE} = \frac{1^2 + 2^2 + 3^2}{3} = \frac{1 + 4 + 9}{3} = \frac{14}{3} \approx 4.67
$$</p>
<p>RMSE is the square root of MSE, bringing the units back to the original scale.</p>
<p>$$
\text{RMSE} = \sqrt{\text{MSE}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>MSE = 4.67</li>
</ul>
<p>$$
\text{RMSE} = \sqrt{4.67} \approx 2.16
$$</p>
<h4 id="r-squared-r">R-Squared (R²)<a hidden class="anchor" aria-hidden="true" href="#r-squared-r">#</a></h4>
<p>R-Squared represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).</p>
<p>$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Total Sum of Squares (TSS) = 100</li>
<li>Residual Sum of Squares (RSS) = 20</li>
</ul>
<p>$$
R^2 = 1 - \frac{20}{100} = 1 - 0.20 = 0.80 , \text{or} , 80\text{%}
$$</p>
<h4 id="mean-absolute-percentage-error-mape">Mean Absolute Percentage Error (MAPE)<a hidden class="anchor" aria-hidden="true" href="#mean-absolute-percentage-error-mape">#</a></h4>
<p>MAPE measures the average absolute percentage error between predicted and actual values.</p>
<p>$$
\text{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Actual values = [100, 200, 300]</li>
<li>Predicted values = [110, 190, 310]</li>
</ul>
<p>$$
\text{MAPE} = \frac{1}{3} \left( \left| \frac{100 - 110}{100} \right| + \left| \frac{200 - 190}{200} \right| + \left| \frac{300 - 310}{300} \right| \right) = \frac{1}{3} \left(0.10 + 0.05 + 0.033\right) = \frac{1}{3} \left(0.183\right) \approx 0.061 , \text{or} , 6.1\text{%}
$$</p>
<h4 id="explained-variance-score">Explained Variance Score<a hidden class="anchor" aria-hidden="true" href="#explained-variance-score">#</a></h4>
<p>Explained variance measures how much of the variance in the target variable is explained by the model.</p>
<p>$$
\text{Explained Variance} = 1 - \frac{\text{Var}(y - \hat{y})}{\text{Var}(y)}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Variance of residuals = 20</li>
<li>Variance of target = 100</li>
</ul>
<p>$$
\text{Explained Variance} = 1 - \frac{20}{100} = 1 - 0.20 = 0.80 , \text{or} , 80\text{%}
$$</p>
<h3 id="computer-vision-metrics">Computer Vision Metrics<a hidden class="anchor" aria-hidden="true" href="#computer-vision-metrics">#</a></h3>
<h4 id="intersection-over-union-iou">Intersection over Union (IoU)<a hidden class="anchor" aria-hidden="true" href="#intersection-over-union-iou">#</a></h4>
<p>IoU is crucial for segmentation and object detection tasks, measuring the overlap between the predicted and ground truth bounding boxes or segments.</p>
<p>$$
\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Area of Overlap = 30</li>
<li>Area of Union = 50</li>
</ul>
<p>$$
\text{IoU} = \frac{30}{50} = 0.60 , \text{or} , 60\text{%}
$$</p>
<h4 id="mean-average-precision-map">Mean Average Precision (mAP)<a hidden class="anchor" aria-hidden="true" href="#mean-average-precision-map">#</a></h4>
<p>mAP is commonly used in object detection, summarizing the precision-recall curve across multiple classes.</p>
<ol>
<li>Calculate the precision-recall curve for each class.</li>
<li>Compute the Average Precision (AP) for each class.</li>
<li>Take the mean of AP values across all classes.</li>
</ol>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>AP values for three classes = [0.8, 0.7, 0.9]</li>
</ul>
<p>$$
\text{mAP} = \frac{0.8 + 0.7 + 0.9}{3} = \frac{2.4}{3} = 0.80 , \text{or} , 80\text{%}
$$</p>
<h4 id="dice-coefficient">Dice Coefficient<a hidden class="anchor" aria-hidden="true" href="#dice-coefficient">#</a></h4>
<p>The Dice Coefficient, similar to IoU, is another metric for segmentation tasks, focusing on the overlap between predicted and ground truth segments.</p>
<p>$$
\text{Dice Coefficient} = \frac{2 \times \text{Area of Overlap}}{\text{Total Area of Predicted} + \text{Total Area of Ground Truth}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Area of Overlap = 30</li>
<li>Total Area of Predicted = 40</li>
<li>Total Area of Ground Truth = 50</li>
</ul>
<p>$$
\text{Dice Coefficient} = \frac{2 \times 30}{40 + 50} = \frac{60}{90} = 0.67 , \text{or} , 67\text{%}
$$</p>
<h4 id="pixel-accuracy">Pixel Accuracy<a hidden class="anchor" aria-hidden="true" href="#pixel-accuracy">#</a></h4>
<p>Pixel Accuracy measures the proportion of correctly classified pixels in the entire image.</p>
<p>$$
\text{Pixel Accuracy} = \frac{\text{Number of Correct Pixels}}{\text{Total Number of Pixels}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>Number of Correct Pixels = 900</li>
<li>Total Number of Pixels = 1000</li>
</ul>
<p>$$
\text{Pixel Accuracy} = \frac{900}{1000} = 0.90 , \text{or} , 90\text{%}
$$</p>
<h4 id="mean-iou-miou">Mean IoU (mIoU)<a hidden class="anchor" aria-hidden="true" href="#mean-iou-miou">#</a></h4>
<p>mIoU is the mean of the Intersection over Union (IoU) for all classes. It is commonly used for semantic segmentation tasks.</p>
<p>$$
\text{mIoU} = \frac{1}{N} \sum_{i=1}^{N} \frac{\text{Area of Overlap}}{\text{Area of Union}}
$$</p>
<p><strong>Example Calculation</strong>:</p>
<ul>
<li>IoU values for three classes = [0.6, 0.7, 0.8]</li>
</ul>
<p>$$
\text{mIoU} = \frac{0.6 + 0.7 + 0.8}{3} = \frac{2.1}{3} = 0.70 , \text{or} , 70\text{%}
$$</p>
<h4 id="structural-similarity-index-ssim">Structural Similarity Index (SSIM)<a hidden class="anchor" aria-hidden="true" href="#structural-similarity-index-ssim">#</a></h4>
<p>SSIM measures the similarity between two images, considering luminance, contrast, and structure.</p>
<p>$$
\text{SSIM}(x, y) = \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
$$</p>
<p><strong>Example Calculation</strong>:
$$Assume ( \mu_x = 100 ), ( \mu_y = 105 ), ( \sigma_x = 15 ), ( \sigma_y = 20 ), ( \sigma_{xy} = 18 ), ( C_1 = 6.5 ), ( C_2 = 58 )$$</p>
<p>$$
\text{SSIM} = \frac{(2 \cdot 100 \cdot 105 + 6.5)(2 \cdot 18 + 58)}{(100^2 + 105^2 + 6.5)(15^2 + 20^2 + 58)}
$$</p>
<p>$$
\text{SSIM} = \frac{(21000 + 6.5)(36 + 58)}{(10000 + 11025 + 6.5)(225 + 400 + 58)}
$$</p>
<p>$$
\text{SSIM} = \frac{21006.5 \cdot 94}{21031.5 \cdot 683} \approx 0.42
$$</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://adilsarsenov.dev/tags/beginner/">Beginner</a></li>
      <li><a href="https://adilsarsenov.dev/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://adilsarsenov.dev/tags/computer-vision/">Computer Vision</a></li>
      <li><a href="https://adilsarsenov.dev/tags/metrics/">Metrics</a></li>
      <li><a href="https://adilsarsenov.dev/tags/evaluation/">Evaluation</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://adilsarsenov.dev/">ASR</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
