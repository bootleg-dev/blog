<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Recurrent Neural Networks - RNN, LSTM, GRU | ASR</title>
<meta name="keywords" content="RNN, GRU, LSTM, Deep Learning">
<meta name="description" content="Vanilla RNN, Long Short-Term Memory, Gated Recurrent Unit.">
<meta name="author" content="">
<link rel="canonical" href="https://adilsarsenov.dev/posts/recurrent-neural-networks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.a72801f0f40a8d7f71aa1cafd1c2f2a993a1f26ca1cfd38fdba65d5b9b0f08a0.css" integrity="sha256-pygB8PQKjX9xqhyv0cLyqZOh8myhz9OP26ZdW5sPCKA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni&#43;Srwf3KK8HyZB9V4809q9TWE="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://adilsarsenov.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://adilsarsenov.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://adilsarsenov.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://adilsarsenov.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://adilsarsenov.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Recurrent Neural Networks - RNN, LSTM, GRU" />
<meta property="og:description" content="Vanilla RNN, Long Short-Term Memory, Gated Recurrent Unit." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adilsarsenov.dev/posts/recurrent-neural-networks/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-21T18:34:44&#43;05:00" />
<meta property="article:modified_time" content="2024-05-21T18:34:44&#43;05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Recurrent Neural Networks - RNN, LSTM, GRU"/>
<meta name="twitter:description" content="Vanilla RNN, Long Short-Term Memory, Gated Recurrent Unit."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://adilsarsenov.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Recurrent Neural Networks - RNN, LSTM, GRU",
      "item": "https://adilsarsenov.dev/posts/recurrent-neural-networks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Recurrent Neural Networks - RNN, LSTM, GRU",
  "name": "Recurrent Neural Networks - RNN, LSTM, GRU",
  "description": "Vanilla RNN, Long Short-Term Memory, Gated Recurrent Unit.",
  "keywords": [
    "RNN", "GRU", "LSTM", "Deep Learning"
  ],
  "articleBody": "Introduction Recurrent Neural Networks are a class of artificial neural networks designed to work with sequential data. Unlike feedforward neural networks, RNNs have loops in them, allowing information to persist. This makes them particularly suited for tasks where context and order matter, such as language translation, speech recognition, and time series prediction.\nThe Architecture of RNNs The basic structure of an RNN consists of a repeating module, often called a cell. This cell takes input from the current time step and the hidden state from the previous time step to produce an output and update the hidden state.\nHere’s a simplified diagram of an RNN cell:\nWhere:\nx[t] is the input at time step t h[t] is the hidden state at time step t y[t] is the output at time step t RNN cell (containing the weights and activation function)\nThe key equations governing the behavior of an RNN are:\nHidden state update: $h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\nOutput calculation: $y_t = W_{hy}h_t + b_y$\nWhere:\n$h_t$ is the hidden state at time t $x_t$ is the input at time t $y_t$ is the output at time t $W_{hh}$, $W_{xh}$, and $W_{hy}$ are weight matrices $b_h$ and $b_y$ are bias vectors $\\tanh$ is the hyperbolic tangent activation function Let’s break down these equations to understand what’s happening:\nHidden state update:\n$W_{hh}h_{t-1}$: This term represents the influence of the previous hidden state. The weight matrix $W_{hh}$ determines how much of the previous state should be retained. $W_{xh}x_t$: This term processes the current input. The weight matrix $W_{xh}$ determines how the current input should influence the hidden state. $b_h$: This is a bias term, allowing the network to shift the activation function. The $\\tanh$ function squashes the sum of these terms to a range between -1 and 1, introducing non-linearity and helping to keep the values stable. Output calculation:\n$W_{hy}h_t$: This transforms the current hidden state into the output space. $b_y$: Another bias term for the output. The power of RNNs comes from their ability to maintain a “memory” of previous inputs through the hidden state, which is updated at each time step and influences future outputs.\nTraining RNNs: Backpropagation Through Time (BPTT) Training RNNs involves a technique called Backpropagation Through Time (BPTT). This is an extension of the standard backpropagation algorithm used in feedforward networks, adapted to work with the temporal nature of RNNs.\nThe basic steps of BPTT are:\nForward pass through the entire sequence Compute the loss Backward pass through the entire sequence Update the weights The loss gradient with respect to the weights is accumulated over all time steps:\n$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial W}$\nWhere $L$ is the total loss and $L_t$ is the loss at time step t.\nBPTT can be thought of as “unrolling” the RNN through time and then applying standard backpropagation. Here’s a more detailed look:\nIn the forward pass, we compute the hidden states and outputs for each time step, storing the results.\nWe then compute a loss function that measures the difference between our predictions and the true values.\nIn the backward pass, we compute the gradient of the loss with respect to each parameter, working backwards from the last time step to the first. This is where the “through time” part comes in – we’re propagating the error back through the temporal structure of the network.\nThe gradients from each time step are summed to get the final gradient for each weight.\nFinally, we update the weights using an optimization algorithm like gradient descent.\nThe key challenge in BPTT is handling long sequences, as the gradients can become very small (vanishing gradient problem) or very large (exploding gradient problem) when propagated over many time steps.\nChallenges in Training RNNs While powerful, RNNs face some challenges during training:\nVanishing Gradients: As the network processes long sequences, gradients can become extremely small, making it difficult for the network to learn long-term dependencies.\nExploding Gradients: Conversely, gradients can also become extremely large, causing instability during training.\nLong-Term Dependencies: Standard RNNs often struggle to capture dependencies over long sequences.\nTo address these challenges, several variants of RNNs have been developed:\nLong Short-Term Memory (LSTM) Networks LSTMs introduce a more complex cell structure with gates to control the flow of information:\nForget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$ Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$ Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$ Cell state update: $C_t = f_t * C_{t-1} + i_t * \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$ Hidden state update: $h_t = o_t * \\tanh(C_t)$ Where $\\sigma$ is the sigmoid function and $*$ denotes element-wise multiplication.\nLSTMs are designed to overcome the vanishing gradient problem. They do this by introducing a new state called the cell state $C_t$, which acts as a conveyor belt of information flowing through the network. The LSTM can add or remove information from the cell state, carefully regulated by structures called gates. Let’s break down each component:\nForget gate ($f_t$): This gate decides what information to throw away from the cell state. It looks at $h_{t-1}$ and $x_t$, and outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$. A 1 represents “keep this” while a 0 represents “forget this.”\nInput gate ($i_t$): This gate decides which new information we’re going to store in the cell state. It has two parts:\nA sigmoid layer that decides which values we’ll update. A tanh layer that creates a vector of new candidate values that could be added to the state. Cell state update: We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t * \\tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.\nOutput gate ($o_t$): This gate decides what we’re going to output. This output will be based on our cell state, but will be a filtered version.\nHidden state update: Finally, we update the hidden state, which is a filtered version of the cell state.\nThe beauty of this architecture is that the cell state provides a direct avenue for information to flow through the network without being substantially changed, which helps with learning long-term dependencies.\nGated Recurrent Units (GRUs) GRUs simplify the LSTM architecture while maintaining many of its advantages:\nUpdate gate: $z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$ Reset gate: $r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$ Candidate hidden state: $\\tilde{h_t} = \\tanh(W \\cdot [r_t * h_{t-1}, x_t])$ Hidden state update: $h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$ GRUs are a simpler variant of LSTMs, combining the forget and input gates into a single “update gate”. They also merge the cell state and hidden state.\nUpdate gate ($z_t$): This gate decides how much of the past information (from previous time steps) needs to be passed along to the future. It can be thought of as the combination of the forget and input gates in an LSTM.\nReset gate ($r_t$): This gate is used to decide how much of the past information to forget.\nCandidate hidden state ($\\tilde{h}_t$): This is a new hidden state candidate, created using the reset gate to determine how much of the past state to incorporate.\nHidden state update: The final hidden state is a linear interpolation between the previous hidden state and the candidate hidden state, with the update gate determining the mix.\nThe main advantage of GRUs over LSTMs is that they’re computationally more efficient due to having fewer parameters. In practice, both GRUs and LSTMs tend to yield comparable results, with the best choice often depending on the specific dataset and task.\n",
  "wordCount" : "1280",
  "inLanguage": "en",
  "datePublished": "2024-05-21T18:34:44.165668+05:00",
  "dateModified": "2024-05-21T18:34:44.165668+05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adilsarsenov.dev/posts/recurrent-neural-networks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ASR",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adilsarsenov.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top"><script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header sticky-header">
    <nav class="nav">
        <div class="logo">
            <a href="https://adilsarsenov.dev/" accesskey="h" title="ASR (Alt + H)">ASR</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://adilsarsenov.dev/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/posts" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://adilsarsenov.dev/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://adilsarsenov.dev/">Home</a>&nbsp;»&nbsp;<a href="https://adilsarsenov.dev/posts/">Posts</a></div>
    <h1 class="post-title">
      Recurrent Neural Networks - RNN, LSTM, GRU
    </h1>
    <div class="post-description">
      Vanilla RNN, Long Short-Term Memory, Gated Recurrent Unit.
    </div>
    <div class="post-meta">


Date: 24 May, 2024

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">‎ Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#the-architecture-of-rnns" aria-label="The Architecture of RNNs">The Architecture of RNNs</a></li>
                <li>
                    <a href="#training-rnns-backpropagation-through-time-bptt" aria-label="Training RNNs: Backpropagation Through Time (BPTT)">Training RNNs: Backpropagation Through Time (BPTT)</a></li>
                <li>
                    <a href="#challenges-in-training-rnns" aria-label="Challenges in Training RNNs">Challenges in Training RNNs</a></li>
                <li>
                    <a href="#long-short-term-memory-lstm-networks" aria-label="Long Short-Term Memory (LSTM) Networks">Long Short-Term Memory (LSTM) Networks</a></li>
                <li>
                    <a href="#gated-recurrent-units-grus" aria-label="Gated Recurrent Units (GRUs)">Gated Recurrent Units (GRUs)</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h3>
<p>Recurrent Neural Networks are a class of artificial neural networks designed to work with <strong>sequential data</strong>.
Unlike feedforward neural networks, RNNs have <strong>loops</strong> in them, allowing information to persist. This makes them
particularly suited for tasks where <strong>context and order</strong> matter, such as language translation, speech recognition,
and time series prediction.</p>
<h3 id="the-architecture-of-rnns">The Architecture of RNNs<a hidden class="anchor" aria-hidden="true" href="#the-architecture-of-rnns">#</a></h3>
<p>The basic structure of an RNN consists of a repeating module, often called a <strong>cell</strong>.
This <strong>cell</strong> takes input from the <strong>current time step</strong> and the <strong>hidden state</strong> from the <strong>previous time step</strong>
to produce an <strong>output and update the hidden state</strong>.</p>
<p>Here&rsquo;s a simplified diagram of an RNN cell:</p>
<p><img loading="lazy" src="/posts/rnns/img1.png" alt="RNNCell"  />
</p>
<p>Where:</p>
<ul>
<li>x[t] is the input at time step t</li>
<li>h[t] is the hidden state at time step t</li>
<li>y[t] is the output at time step t</li>
</ul>
<p>RNN cell (containing the weights and activation function)</p>
<p>The <strong>key equations</strong> governing the behavior of an RNN are:</p>
<ol>
<li>
<p>Hidden state update:
$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$</p>
</li>
<li>
<p>Output calculation:
$y_t = W_{hy}h_t + b_y$</p>
</li>
</ol>
<p>Where:</p>
<ul>
<li>$h_t$ is the hidden state at time t</li>
<li>$x_t$ is the input at time t</li>
<li>$y_t$ is the output at time t</li>
<li>$W_{hh}$, $W_{xh}$, and $W_{hy}$ are weight matrices</li>
<li>$b_h$ and $b_y$ are bias vectors</li>
<li>$\tanh$ is the hyperbolic tangent activation function</li>
</ul>
<p>Let&rsquo;s break down these equations to understand what&rsquo;s happening:</p>
<ol>
<li>
<p>Hidden state update:</p>
<ul>
<li>$W_{hh}h_{t-1}$: This term represents the influence of the previous hidden state. The weight matrix $W_{hh}$ determines how much of the previous state should be retained.</li>
<li>$W_{xh}x_t$: This term processes the current input. The weight matrix $W_{xh}$ determines how the current input should influence the hidden state.</li>
<li>$b_h$: This is a bias term, allowing the network to shift the activation function.</li>
<li>The $\tanh$ function squashes the sum of these terms to a range between -1 and 1, introducing non-linearity and helping to keep the values stable.</li>
</ul>
</li>
<li>
<p>Output calculation:</p>
<ul>
<li>$W_{hy}h_t$: This transforms the current hidden state into the output space.</li>
<li>$b_y$: Another bias term for the output.</li>
</ul>
</li>
</ol>
<p>The power of RNNs comes from their <strong>ability to maintain a &ldquo;memory&rdquo; of previous inputs through the hidden state, which is updated at each time step and influences future outputs</strong>.</p>
<h3 id="training-rnns-backpropagation-through-time-bptt">Training RNNs: Backpropagation Through Time (BPTT)<a hidden class="anchor" aria-hidden="true" href="#training-rnns-backpropagation-through-time-bptt">#</a></h3>
<p>Training RNNs involves a technique called <strong>Backpropagation Through Time (BPTT)</strong>.
This is an extension of the standard backpropagation algorithm used in feedforward networks, adapted to work with the temporal nature of RNNs.</p>
<p>The basic steps of <strong>BPTT</strong> are:</p>
<ol>
<li>Forward pass through the entire sequence</li>
<li>Compute the loss</li>
<li>Backward pass through the entire sequence</li>
<li>Update the weights</li>
</ol>
<p>The loss gradient with respect to the weights is accumulated over all time steps:</p>
<p>$\frac{\partial L}{\partial W} = \sum_{t=1}^T \frac{\partial L_t}{\partial W}$</p>
<p>Where $L$ is the total loss and $L_t$ is the loss at time step t.</p>
<p>BPTT can be thought of as <strong>&ldquo;unrolling&rdquo; the RNN through time and then applying standard backpropagation</strong>. Here&rsquo;s a more detailed look:</p>
<ol>
<li>
<p>In the forward pass, we compute the hidden states and outputs for each time step, storing the results.</p>
</li>
<li>
<p>We then compute a loss function that measures the difference between our predictions and the true values.</p>
</li>
<li>
<p>In the backward pass, we compute the gradient of the loss with respect to each parameter, working backwards from the last time step to the first. This is where the <strong>&ldquo;through time&rdquo;</strong> part comes in – we&rsquo;re propagating the error back through the temporal structure of the network.</p>
</li>
<li>
<p>The gradients from each time step are summed to get the final gradient for each weight.</p>
</li>
<li>
<p>Finally, we update the weights using an optimization algorithm like gradient descent.</p>
</li>
</ol>
<p>The key challenge in <strong>BPTT is handling long sequences</strong>, as the gradients can become very small <strong>(vanishing gradient problem)</strong> or very large <strong>(exploding gradient problem)</strong> when propagated over many time steps.</p>
<h3 id="challenges-in-training-rnns">Challenges in Training RNNs<a hidden class="anchor" aria-hidden="true" href="#challenges-in-training-rnns">#</a></h3>
<p>While powerful, RNNs face some challenges during training:</p>
<ol>
<li>
<p><strong>Vanishing Gradients</strong>: As the network processes long sequences, gradients can become extremely small, making it difficult for the network to learn long-term dependencies.</p>
</li>
<li>
<p><strong>Exploding Gradients</strong>: Conversely, gradients can also become extremely large, causing instability during training.</p>
</li>
<li>
<p><strong>Long-Term Dependencies</strong>: Standard RNNs often struggle to capture dependencies over long sequences.</p>
</li>
</ol>
<p>To address these challenges, several <strong>variants of RNNs have been developed</strong>:</p>
<h3 id="long-short-term-memory-lstm-networks">Long Short-Term Memory (LSTM) Networks<a hidden class="anchor" aria-hidden="true" href="#long-short-term-memory-lstm-networks">#</a></h3>
<p>LSTMs introduce a more complex <strong>cell structure with gates</strong> to control the flow of information:</p>
<p><img loading="lazy" src="/posts/rnns/img2.png" alt="LSTMCell"  />
</p>
<ul>
<li>Forget gate: $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$</li>
<li>Input gate: $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$</li>
<li>Output gate: $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$</li>
<li>Cell state update: $C_t = f_t * C_{t-1} + i_t * \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$</li>
<li>Hidden state update: $h_t = o_t * \tanh(C_t)$</li>
</ul>
<p>Where $\sigma$ is the sigmoid function and $*$ denotes element-wise multiplication.</p>
<p>LSTMs are designed to overcome the <strong>vanishing gradient problem</strong>. They do this by introducing a new state
called the <strong>cell state</strong> $C_t$, which acts as a conveyor belt of information flowing through the network.
The LSTM can <strong>add or remove information</strong> from the cell state, carefully regulated by structures called <strong>gates</strong>.
Let&rsquo;s break down each component:</p>
<ol>
<li>
<p><strong>Forget gate</strong> ($f_t$): This gate decides what information to throw away from the cell state. It looks at $h_{t-1}$ and $x_t$, and outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$. A 1 represents &ldquo;keep this&rdquo; while a 0 represents &ldquo;forget this.&rdquo;</p>
</li>
<li>
<p><strong>Input gate</strong> ($i_t$): This gate decides which new information we&rsquo;re going to store in the cell state. It has two parts:</p>
<ul>
<li>A sigmoid layer that decides which values we&rsquo;ll update.</li>
<li>A tanh layer that creates a vector of new candidate values that could be added to the state.</li>
</ul>
</li>
<li>
<p><strong>Cell state update</strong>: We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t * \tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.</p>
</li>
<li>
<p><strong>Output gate</strong> ($o_t$): This gate decides what we&rsquo;re going to output. This output will be based on our cell state, but will be a filtered version.</p>
</li>
<li>
<p><strong>Hidden state update</strong>: Finally, we update the hidden state, which is a filtered version of the cell state.</p>
</li>
</ol>
<p>The beauty of this architecture is that the cell state provides a direct avenue for information to flow through the
network without being substantially changed, which helps with learning long-term dependencies.</p>
<h3 id="gated-recurrent-units-grus">Gated Recurrent Units (GRUs)<a hidden class="anchor" aria-hidden="true" href="#gated-recurrent-units-grus">#</a></h3>
<p>GRUs simplify the LSTM architecture while maintaining many of its advantages:</p>
<p><img loading="lazy" src="/posts/rnns/img3.png" alt="GRUCell"  />
</p>
<ul>
<li>Update gate: $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$</li>
<li>Reset gate: $r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$</li>
<li>Candidate hidden state: $\tilde{h_t} = \tanh(W \cdot [r_t * h_{t-1}, x_t])$</li>
<li>Hidden state update: $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$</li>
</ul>
<p>GRUs are a <strong>simpler variant of LSTMs</strong>, combining the <strong>forget</strong> and <strong>input gates</strong> into a <strong>single &ldquo;update gate&rdquo;</strong>.
They also merge the <strong>cell state and hidden state</strong>.</p>
<ol>
<li>
<p><strong>Update gate</strong> ($z_t$): This gate decides how much of the past information (from previous time steps) needs to be passed along to the future. It can be thought of as the combination of the forget and input gates in an LSTM.</p>
</li>
<li>
<p><strong>Reset gate</strong> ($r_t$): This gate is used to decide how much of the past information to forget.</p>
</li>
<li>
<p><strong>Candidate hidden state</strong> ($\tilde{h}_t$): This is a new hidden state candidate, created using the reset gate to determine how much of the past state to incorporate.</p>
</li>
<li>
<p><strong>Hidden state update</strong>: The final hidden state is a linear interpolation between the previous hidden state and the candidate hidden state, with the update gate determining the mix.</p>
</li>
</ol>
<p>The main advantage of GRUs over LSTMs is that they&rsquo;re <strong>computationally more efficient due to having fewer parameters</strong>.
In practice, both GRUs and LSTMs tend to yield <strong>comparable results</strong>, with the best choice often depending on the specific dataset and task.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://adilsarsenov.dev/tags/rnn/">RNN</a></li>
      <li><a href="https://adilsarsenov.dev/tags/gru/">GRU</a></li>
      <li><a href="https://adilsarsenov.dev/tags/lstm/">LSTM</a></li>
      <li><a href="https://adilsarsenov.dev/tags/deep-learning/">Deep Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://adilsarsenov.dev/">ASR</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
