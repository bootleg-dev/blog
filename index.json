[{"content":"Introduction Vision Transformers (ViT) have emerged as a groundbreaking architecture in the field of computer vision, challenging the long-standing dominance of Convolutional Neural Networks (CNNs).\nIntroduced by Alexey Dosovitskiy and his team at Google Research in 2020, ViTs apply the transformer architecture, originally designed for natural language processing tasks, to image recognition problems.\nTraditional CNNs have been the cornerstone of image processing for years, but they have limitations in capturing global context and long-range dependencies. ViTs address these limitations by leveraging the self-attention mechanism, which allows the model to consider the entire image at once, rather than just local regions.\nBackground: From CNNs to Transformers To appreciate the significance of Vision Transformers, it\u0026rsquo;s crucial to understand the context in which they emerged. For nearly a decade, Convolutional Neural Networks (CNNs) have been the go-to architecture for image-related tasks. CNNs excel at capturing local spatial relationships in images through their use of convolutional filters. However, they struggle with modeling long-range dependencies efficiently.\nCNNs work by applying a series of convolutional filters to an image, each filter looking for specific features like edges, textures, or more complex patterns. This approach is inherently local – each layer in a CNN only looks at a small portion of the input at a time. While this is effective for many tasks, it can miss important global context.\nTransformers, on the other hand, were initially designed for sequence-to-sequence tasks in natural language processing. Their key innovation is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input when processing each element. This mechanism enables transformers to capture long-range dependencies effectively.\nIn the context of language, this means a transformer can easily relate words at the beginning of a sentence to words at the end, something that\u0026rsquo;s more challenging for traditional recurrent neural networks. When applied to images, this translates to the ability to relate distant parts of an image, capturing global context more effectively than CNNs.\nThe success of transformers in NLP tasks prompted researchers to explore their potential in other domains, including computer vision. This exploration led to the development of Vision Transformers, which adapt the transformer architecture to work with image data.\nArchitecture of Vision Transformers Vision Transformers adapt the transformer architecture to work with image data. The key steps in this process are:\nImage Patching: The input image is divided into fixed-size patches. Linear Embedding: Each patch is flattened and linearly embedded. Position Embedding: Positional information is added to the patch embeddings. Transformer Encoder: The embedded patches are processed by a standard transformer encoder. Classification Head: The output of the encoder is used for classification. Image Patching The first step in processing an image with a Vision Transformer is to divide it into fixed-size patches. Given an image $x \\in \\mathbb{R}^{H \\times W \\times C}$, where $H$, $W$, and $C$ are the height, width, and number of channels respectively, we split it into $N$ patches where $N = HW/P^2$. Each patch $x_p \\in \\mathbb{R}^{P \\times P \\times C}$ has a resolution of $P \\times P$. This patching operation can be seen as a form of tokenization, similar to how words are tokenized in NLP tasks. Each patch becomes a \u0026ldquo;visual word\u0026rdquo; that the transformer will process.\nThe choice of patch size is an important hyperparameter. Smaller patches allow for finer-grained analysis but increase computational complexity, while larger patches reduce complexity but may lose fine details. Typical patch sizes range from 14x14 to 32x32 pixels.\nFor example, if we have a 224x224 pixel RGB image and choose a patch size of 16x16, we would end up with 196 patches (14x14 grid of patches), each represented as a 768-dimensional vector (16 * 16 * 3 = 768).\nLinear Embedding After patching, each patch is flattened and linearly projected to a D-dimensional embedding space. This is done using a learnable linear projection $E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$ The resulting patch embeddings are denoted as:\n$$z_0 = [x_p^1E; x_p^2E; \u0026hellip; ; x_p^NE]$$\nwhere $x_p^i$ is the i-th flattened patch and $z_0 \\in \\mathbb{R}^{N \\times D}$ is the sequence of patch embeddings.\nThis linear embedding serves multiple purposes:\nIt allows the model to learn a meaningful representation of the image patches in a high-dimensional space. It maps the variable-sized patches (depending on the image size) to a fixed-dimensional space that the transformer can process. It can be seen as learning a set of filters, similar to the first layer of a CNN, but applied globally to each patch. The dimension D is typically chosen to match the internal dimension of the transformer model, often 768 or 1024 in practice.\nPosition Embedding Unlike CNNs, which inherently capture spatial information through their convolutional operations, transformers don\u0026rsquo;t have a built-in sense of spatial relationships. To compensate for this, position embeddings are added to the patch embeddings.\nA learnable position embedding $E_{pos} \\in \\mathbb{R}^{(N+1) \\times D}$ is added to the patch embeddings. The \u0026quot;+1\u0026quot; in the dimension accounts for a special \u0026ldquo;classification token\u0026rdquo; that\u0026rsquo;s prepended to the sequence of patch embeddings.\nThis results in:\n$$z_0 = [x_{class}; x_p^1E; x_p^2E; \u0026hellip;; x_p^NE] + E_{pos}$$\nwhere $x_{class}$ is the learnable classification token.\nThe position embeddings play a crucial role:\nThey provide the model with information about the spatial arrangement of the patches. Unlike in NLP transformers where positions are usually encoded using fixed sinusoidal functions, ViTs typically use learnable position embeddings, allowing the model to adapt to the 2D structure of images. The addition of position embeddings to the patch embeddings allows the model to distinguish between identical patches at different locations in the image. The classification token ($x_{class}$) is a special learned vector that\u0026rsquo;s prepended to the sequence of patch embeddings. Its final representation after passing through the transformer encoder is used for classification tasks, serving a similar purpose to the [CLS] token in BERT for NLP tasks.\nTransformer Encoder The core of the Vision Transformer is the transformer encoder. It consists of alternating layers of multihead self-attention (MSA) and multilayer perceptrons (MLP). Layer normalization (LN) is applied before each block, and residual connections are employed around each block.\nThe computation in the L-layer transformer encoder proceeds as follows:\n$$\\begin{aligned} z_0\u0026rsquo; \u0026amp;= [x_{class}; x_p^1E; x_p^2E; \u0026hellip;; x_p^NE] + E_{pos} \\end{aligned}$$\n$$\\begin{aligned} z_l\u0026rsquo; \u0026amp;= MSA(LN(z_{l-1})) + z_{l-1}, \u0026amp;l = 1\u0026hellip;L \\end{aligned}$$\n$$\\begin{aligned} z_l \u0026amp;= MLP(LN(z_l\u0026rsquo;)) + z_l\u0026rsquo;, \u0026amp;l = 1\u0026hellip;L \\end{aligned}$$\nThe multihead self-attention operation is the key component that allows the model to capture relationships between different parts of the image. For each attention head, the input is projected into query (Q), key (K), and value (V) vectors. The attention weights are computed as:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nwhere $d_k$ is the dimension of the key vectors. The $\\frac{1}{\\sqrt{d_k}}$ scaling factor is used to counteract the effect of the dot product growing large in magnitude for high dimensions.\nLet\u0026rsquo;s break down the transformer encoder further:\nMultihead Self-Attention (MSA): This allows the model to attend to different parts of the input simultaneously. Each head can focus on different relationships between patches.\nLayer Normalization (LN): This helps stabilize the learning process by normalizing the inputs to each layer.\nMultilayer Perceptron (MLP): This is typically a simple feed-forward network applied to each position separately and identically. It allows for non-linear transformations of the features.\nResidual Connections: These help in training deep networks by allowing gradients to flow more easily through the network.\nThe self-attention mechanism is particularly powerful because it allows each patch to interact with every other patch, capturing global relationships in the image. This is in contrast to CNNs, where each layer only looks at a local neighborhood of pixels.\nClassification Head After the transformer encoder processes the sequence of patch embeddings, the final classification is performed using the representation of the classification token. A simple linear layer is typically used as the classification head:\n$$y = MLP(z_L^0)$$\nwhere $z_L^0$ is the final hidden state corresponding to the classification token.\nThe classification head is straightforward compared to the rest of the architecture. It takes the final representation of the classification token, which has aggregated information from the entire image through the self-attention process, and maps it to the output classes.\nThis simplicity is part of the elegance of the ViT architecture – all the heavy lifting is done by the transformer encoder, and the classification is a simple linear projection of the resulting representation.\nTraining and Fine-tuning Vision Transformers are typically pre-trained on large datasets and then fine-tuned on specific tasks. The pre-training is often done using a supervised approach on large-scale datasets like ImageNet-21k or JFT-300M. During pre-training, the model learns to extract meaningful features from images that can be useful for a wide range of tasks.\nThe pre-training process is crucial for ViTs, perhaps even more so than for CNNs. This is because ViTs lack the inductive biases that CNNs have (such as translation invariance), so they need to learn these properties from data. Pre-training on a large, diverse dataset allows the ViT to learn general visual features that can be applied to many different tasks.\nThe fine-tuning process for ViTs is similar to that of other pre-trained models:\nThe pre-trained ViT is loaded, usually without the final classification layer. A new classification layer is added, with the number of outputs matching the number of classes in the target task. The model is then trained on the target dataset. Often, a lower learning rate is used for the pre-trained weights, with a higher learning rate for the new classification layer. One of the strengths of ViTs is their ability to transfer well to a wide range of tasks with minimal fine-tuning. This is likely due to the global nature of the features they learn during pre-training.\nAdvantages of Vision Transformers Vision Transformers offer several advantages over traditional CNN architectures:\nGlobal Context: The self-attention mechanism allows ViTs to capture long-range dependencies and global context more effectively than CNNs. This means ViTs can more easily relate distant parts of an image, which can be crucial for tasks that require understanding the overall scene or object relationships.\nScalability: ViTs have shown impressive scaling properties, with performance continuing to improve as model size and training data increase. This is similar to what has been observed with language models, where larger models trained on more data consistently perform better.\nTransfer Learning: Pre-trained ViTs have demonstrated strong transfer learning capabilities, performing well on a wide range of tasks with minimal fine-tuning. This makes them versatile and potentially more cost-effective for organizations dealing with multiple vision tasks.\nInterpretability: The attention maps produced by ViTs can provide insights into which parts of the image the model is focusing on for its decisions. This can be valuable for understanding and debugging model behavior.\nUnified Architecture: ViTs provide a unified architecture that can be applied to various vision tasks beyond just classification, including object detection and segmentation. This simplifies the model design process across different tasks.\nEfficiency at Scale: While ViTs can be computationally expensive to train, they can be more efficient than CNNs for very large models. This is because self-attention operations can be optimized more effectively on modern hardware than convolutions for large model sizes.\nChallenges and Limitations Despite their advantages, Vision Transformers also face some challenges:\nData Hunger: ViTs typically require larger datasets for training compared to CNNs, especially when training from scratch. This is because they need to learn basic visual features that are built into the architecture of CNNs. For smaller datasets, CNNs often still outperform ViTs unless specific techniques are used to address this limitation.\nComputational Cost: The self-attention mechanism in ViTs has a quadratic complexity with respect to the number of patches, which can be computationally expensive for high-resolution images. This means that processing very large images or scaling to very high resolutions can be challenging.\nLack of Inductive Biases: Unlike CNNs, which have built-in inductive biases for processing visual data (like translation invariance), ViTs need to learn these properties from data, which can require more training. This is why pre-training on large datasets is so important for ViTs.\nPositional Information: While position embeddings help, capturing and utilizing spatial relationships as effectively as CNNs remains a challenge for ViTs. The discrete nature of the patch-based approach can sometimes lead to artifacts or reduced performance on tasks that require fine-grained spatial understanding.\nModel Size: Competitive ViT models are often larger than their CNN counterparts, which can make them challenging to deploy in resource-constrained environments like mobile devices.\nTraining Instability: ViTs can sometimes be more challenging to train than CNNs, requiring careful tuning of hyperparameters and learning rates. The lack of inductive biases can lead to more pronounced overfitting on smaller datasets.\nResearchers are actively working on addressing these limitations, leading to numerous variations and improvements on the original ViT architecture.\nRecent Developments and Variations Since the introduction of the original ViT, numerous variations and improvements have been proposed:\nDeiT (Data-efficient Image Transformers): This variant introduces a teacher-student strategy to train ViTs more efficiently on smaller datasets. DeiT uses a CNN as a teacher to provide additional supervision during training, allowing the ViT to learn more effectively from limited data. This addresses one of the main limitations of the original ViT – its data hunger.\nSwin Transformer: This hierarchical approach uses shifted windows to compute self-attention, allowing for better efficiency and applicability to dense prediction tasks. The Swin Transformer computes self-attention within local windows, and these windows are shifted between successive layers. This approach reduces computational complexity and makes it easier to apply ViTs to tasks like object detection and segmentation.\nMLP-Mixer: This architecture replaces the self-attention layers with simple MLPs, demonstrating that the patch-based approach, rather than self-attention, might be the key innovation of ViTs. MLP-Mixer alternates between MLPs applied across channels and MLPs applied across spatial locations. This simplification can lead to faster training and inference while maintaining competitive performance.\nCoAtNet: This hybrid approach combines convolutions and self-attention, aiming to get the best of both worlds. It uses convolutions in the earlier layers to efficiently process low-level features, and self-attention in the later layers to capture global context. This combines the efficiency and inductive biases of CNNs with the long-range modeling capabilities of transformers.\nViT-G: A giant Vision Transformer model with 1.8 billion parameters, demonstrating the scalability of the architecture. ViT-G shows that, like in language models, scaling up ViTs can lead to significant performance improvements. However, the computational resources required for such large models are substantial.\nPyramid Vision Transformer (PVT): This variant introduces a pyramid structure to ViTs, similar to the feature pyramid networks used in CNNs\n","permalink":"https://adilsarsenov.dev/posts/visual-transformers/","summary":"Introduction Vision Transformers (ViT) have emerged as a groundbreaking architecture in the field of computer vision, challenging the long-standing dominance of Convolutional Neural Networks (CNNs).\nIntroduced by Alexey Dosovitskiy and his team at Google Research in 2020, ViTs apply the transformer architecture, originally designed for natural language processing tasks, to image recognition problems.\nTraditional CNNs have been the cornerstone of image processing for years, but they have limitations in capturing global context and long-range dependencies.","title":"Vision Transformers - ViT"},{"content":"Introduction The Transformer architecture, introduced in the seminal paper \u0026ldquo;Attention Is All You Need\u0026rdquo; by Vaswani et al. in 2017, has revolutionized the field of natural language processing (NLP) and beyond with its key innovation: the Attention mechanism.\nThe Transformer Architecture Overview The Transformer is a neural network architecture designed to handle sequential data, particularly in tasks like machine translation, text summarization, and language understanding. Unlike its predecessors (RNNs and LSTMs), Transformers process entire sequences simultaneously, allowing for more parallelization and, consequently, faster training on larger datasets.\nTo understand why this is revolutionary, consider how we typically process language. Traditionally, we\u0026rsquo;d look at words one by one, trying to understand each in the context of what came before. Transformers, however, look at the entire sentence at once, weighing the importance of each word in relation to all the others. This is akin to understanding the meaning of a sentence by considering all words simultaneously rather than sequentially.\nThe architecture consists of two main components:\nEncoder: Processes the input sequence Decoder: Generates the output sequence Both the encoder and decoder are composed of a stack of identical layers, each containing two sub-layers:\nMulti-Head Attention mechanism Position-wise Fully Connected Feed-Forward Network Key Components 1. Input Embedding Before processing, input tokens (which could be words or subwords) are converted into continuous vector representations. This is typically done using learned embeddings. Mathematically, this can be represented as:\n$E = XW_e$\nWhere:\n$E$ is the embedding matrix $X$ is the one-hot encoded input $W_e$ is the learned embedding weight matrix This formula represents the process of mapping discrete tokens to continuous vector spaces. Each row of $W_e$ corresponds to the embedding vector for a specific token in the vocabulary.\nWhy is this important? Words (or tokens) in language don\u0026rsquo;t inherently have mathematical meaning. By converting them to vectors, we\u0026rsquo;re representing them in a way that captures semantic relationships. For instance, in a well-trained embedding space, the vectors for \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; might be close to each other, reflecting their related meanings.\n2. Positional Encoding One key difference between Transformers and traditional sequence models is that Transformers don\u0026rsquo;t inherently understand the order of the input sequence. To address this, we add positional encodings to the input embeddings. This injects information about the position of tokens in the sequence.\nThe positional encoding is calculated using sine and cosine functions:\n$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{model}})$ $PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$\nWhere:\n$pos$ is the position in the sequence $i$ is the dimension $d_{model}$ is the embedding dimension Why use sine and cosine functions? These functions have a beautiful property: for any fixed offset k, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. This allows the model to easily learn to attend to relative positions, which is crucial for understanding language structure.\nThis encoding allows the model to distinguish between different positions in the sequence. Without it, the model would treat \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; identically, which would be problematic for understanding meaning!\n3. Multi-Head Attention This is the core innovation of the Transformer architecture, explained in the next section. The key idea is that it allows the model to focus on different parts of the input when producing each part of the output, much like how we might focus on different words when translating a sentence.\n4. Feed-Forward Networks Each attention layer is followed by a position-wise feed-forward network. This consists of two linear transformations with a ReLU activation in between:\n$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\nWhere:\n$x$ is the input to the feed-forward network $W_1$, $W_2$ are weight matrices $b_1$, $b_2$ are bias vectors This feed-forward network is applied to each position separately and identically. It allows the model to introduce non-linearity and increase the representational power of the network. The ReLU activation helps in learning complex patterns.\nWhy is this necessary? The attention mechanism is inherently linear. By adding this non-linear feed-forward network, we\u0026rsquo;re giving the model the ability to approximate more complex functions, which is crucial for learning intricate patterns in language.\n5. Layer Normalization and Residual Connections After each sub-layer (attention and feed-forward), layer normalization is applied. Additionally, residual connections are used around each sub-layer:\n$LayerNorm(x + Sublayer(x))$\nLayer normalization helps stabilize the learning process by normalizing the inputs across the features. The residual connections allow for deeper networks by providing a direct path for gradients to flow backwards, mitigating the vanishing gradient problem.\nThis combination of normalization and residual connections is crucial for training very deep networks. It helps the model learn stably even with many layers, which is key to the power of large language models like GPT-3.\nThe Attention Mechanism Intuition The attention mechanism is the heart of the Transformer architecture. But what exactly is Attention? In essence, it\u0026rsquo;s a way for the model to focus on different parts of the input when producing each part of the output.\nThink about how you read a complex sentence. You don\u0026rsquo;t give equal importance to all words; you focus more on some words to understand the overall meaning. That\u0026rsquo;s essentially what attention does for the model.\nScaled Dot-Product Attention The basic building block of attention in Transformers is called Scaled Dot-Product Attention. It\u0026rsquo;s defined as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nWhere:\n$Q$: Query matrix $K$: Key matrix $V$: Value matrix $d_k$: Dimension of the keys Let\u0026rsquo;s break this down step by step:\nCompute the dot product of the query with all keys: $QK^T$. This gives us a measure of how much each key should be attended to for this particular query. Scale the result by $\\sqrt{d_k}$ to counteract the effect of large dot products in high dimensions. Without this scaling, for large values of $d_k$, the dot products get large, pushing the softmax function into regions where it has extremely small gradients. Apply a softmax function to obtain the weights on the values. This converts the scores to probabilities, ensuring they sum to 1. Multiply the values by their corresponding weights from the softmax. This gives us a weighted sum of the values, where the weights determine how much each value contributes to the output. The intuition here is that we\u0026rsquo;re deciding how much to attend to different parts of the input (represented by the keys and values) based on what we\u0026rsquo;re looking for (the query).\nMulti-Head Attention Instead of performing a single attention function, the Transformer uses multi-head attention. This allows the model to jointly attend to information from different representation subspaces at different positions.\nMulti-head attention consists of several attention layers running in parallel:\n$MultiHead(Q, K, V) = Concat(head_1, \u0026hellip;, head_h)W^O$\nwhere $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\nHere, the $W$ matrices are learned parameters. Each head can learn to attend to different aspects of the input, allowing for a richer representation.\nWhy is this useful?\nDifferent heads can learn to focus on different aspects of the relationship between words. One head might learn to focus on syntactic relationships, while another might focus on semantic relationships. This multi-faceted approach allows the model to capture a more nuanced understanding of the input.\nTypes of Attention in Transformers Encoder Self-Attention: Each position in the encoder attends to all positions in the previous encoder layer. This allows each token to gather information from all other tokens in the input sequence.\nDecoder Self-Attention: Each position in the decoder attends to all previous positions in the decoder. This is made causal (masked) to prevent positions from attending to subsequent positions, which is necessary for autoregressive generation.\nEncoder-Decoder Attention: Each position in the decoder attends to all positions in the encoder. This allows the decoder to focus on relevant parts of the input sequence for each decoding step.\nThese different types of attention allow the model to capture various types of relationships within and between sequences, enabling it to perform complex language tasks.\nMathematical Deep Dive Let\u0026rsquo;s break down the mathematics of the attention mechanism even further:\nQuery, Key, and Value Calculation: For each attention head $i$:\n$Q_i = XW_i^Q$ $K_i = XW_i^K$ $V_i = XW_i^V$\nWhere $X$ is the input, and $W$ matrices are learned parameters. This linear transformation allows each head to project the input into a different subspace, enabling the model to capture different types of relationships in the data.\nAttention Scores:\n$S_i = \\frac{Q_iK_i^T}{\\sqrt{d_k}}$\nThis computes a similarity score between each query and key. The scaling factor $\\sqrt{d_k}$ prevents the dot products from growing too large in magnitude, which could push the softmax function into regions with very small gradients.\nAttention Weights:\n$A_i = softmax(S_i)$\nThe softmax function normalizes the scores, converting them into a probability distribution. This determines how much each value will contribute to the output. The softmax ensures that the weights sum to 1 for each query.\nOutput of Each Head:\n$H_i = A_iV_i$\nThis weighted sum of the values represents the output of each attention head. It combines the values based on the attention weights, allowing the model to focus on relevant information for each position.\nConcatenation and Final Projection:\n$MultiHead = Concat(H_1, \u0026hellip;, H_h)W^O$\nThe outputs of all heads are concatenated and projected to the desired dimension using another learned weight matrix $W^O$. This final step combines the information from all attention heads into a single representation.\nThis mathematical formulation allows the model to dynamically focus on different parts of the input for each part of the output, enabling it to capture complex relationships in the data.\nWhy Transformers Work So Well? Parallelization: Unlike RNNs, Transformers can process entire sequences in parallel, leading to faster training. This is because the self-attention operation can be computed for all positions simultaneously.\nLong-range Dependencies: The attention mechanism allows the model to directly connect distant positions, mitigating the vanishing gradient problem that plagued RNNs. Every output element is connected to every input element, and the weighted sum of these connections is what allows the model to easily learn long-range dependencies.\nFlexible Context: By using attention, the model can dynamically focus on relevant parts of the input, regardless of their position. This is particularly useful in tasks like translation, where the order of words might change between languages.\nRich Representations: Multi-head attention allows the model to capture different types of relationships in the data simultaneously. Each head can specialize in different aspects of the input, providing a more comprehensive representation.\nInterpretability: The attention weights can be visualized to understand which parts of the input the model is focusing on for each output. This provides some level of interpretability, which is often lacking in deep neural networks.\nScalability: The architecture of Transformers scales well with more data and larger model sizes. This has led to the development of increasingly large and powerful models like GPT-3 and BERT.\nThe Transformer architecture and its attention mechanism have become the backbone of many state-of-the-art models in NLP and beyond. Their ability to process sequential data efficiently while capturing complex relationships has led to breakthroughs in various applications. As research continues, we\u0026rsquo;re seeing Transformers adapted for vision tasks, multi-modal learning, and even scientific applications like protein folding prediction. Understanding the fundamentals of this architecture is crucial for anyone working in modern machine learning and artificial intelligence.\n","permalink":"https://adilsarsenov.dev/posts/attention-transformers/","summary":"Introduction The Transformer architecture, introduced in the seminal paper \u0026ldquo;Attention Is All You Need\u0026rdquo; by Vaswani et al. in 2017, has revolutionized the field of natural language processing (NLP) and beyond with its key innovation: the Attention mechanism.\nThe Transformer Architecture Overview The Transformer is a neural network architecture designed to handle sequential data, particularly in tasks like machine translation, text summarization, and language understanding. Unlike its predecessors (RNNs and LSTMs), Transformers process entire sequences simultaneously, allowing for more parallelization and, consequently, faster training on larger datasets.","title":"Transformer Architecture"},{"content":"Introduction Recurrent Neural Networks are a class of artificial neural networks designed to work with sequential data. Unlike feedforward neural networks, RNNs have loops in them, allowing information to persist. This makes them particularly suited for tasks where context and order matter, such as language translation, speech recognition, and time series prediction.\nThe Architecture of RNNs The basic structure of an RNN consists of a repeating module, often called a cell. This cell takes input from the current time step and the hidden state from the previous time step to produce an output and update the hidden state.\nHere\u0026rsquo;s a simplified diagram of an RNN cell:\nWhere:\nx[t] is the input at time step t h[t] is the hidden state at time step t y[t] is the output at time step t RNN cell (containing the weights and activation function)\nThe key equations governing the behavior of an RNN are:\nHidden state update: $h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\nOutput calculation: $y_t = W_{hy}h_t + b_y$\nWhere:\n$h_t$ is the hidden state at time t $x_t$ is the input at time t $y_t$ is the output at time t $W_{hh}$, $W_{xh}$, and $W_{hy}$ are weight matrices $b_h$ and $b_y$ are bias vectors $\\tanh$ is the hyperbolic tangent activation function Let\u0026rsquo;s break down these equations to understand what\u0026rsquo;s happening:\nHidden state update:\n$W_{hh}h_{t-1}$: This term represents the influence of the previous hidden state. The weight matrix $W_{hh}$ determines how much of the previous state should be retained. $W_{xh}x_t$: This term processes the current input. The weight matrix $W_{xh}$ determines how the current input should influence the hidden state. $b_h$: This is a bias term, allowing the network to shift the activation function. The $\\tanh$ function squashes the sum of these terms to a range between -1 and 1, introducing non-linearity and helping to keep the values stable. Output calculation:\n$W_{hy}h_t$: This transforms the current hidden state into the output space. $b_y$: Another bias term for the output. The power of RNNs comes from their ability to maintain a \u0026ldquo;memory\u0026rdquo; of previous inputs through the hidden state, which is updated at each time step and influences future outputs.\nTraining RNNs: Backpropagation Through Time (BPTT) Training RNNs involves a technique called Backpropagation Through Time (BPTT). This is an extension of the standard backpropagation algorithm used in feedforward networks, adapted to work with the temporal nature of RNNs.\nThe basic steps of BPTT are:\nForward pass through the entire sequence Compute the loss Backward pass through the entire sequence Update the weights The loss gradient with respect to the weights is accumulated over all time steps:\n$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial W}$\nWhere $L$ is the total loss and $L_t$ is the loss at time step t.\nBPTT can be thought of as \u0026ldquo;unrolling\u0026rdquo; the RNN through time and then applying standard backpropagation. Here\u0026rsquo;s a more detailed look:\nIn the forward pass, we compute the hidden states and outputs for each time step, storing the results.\nWe then compute a loss function that measures the difference between our predictions and the true values.\nIn the backward pass, we compute the gradient of the loss with respect to each parameter, working backwards from the last time step to the first. This is where the \u0026ldquo;through time\u0026rdquo; part comes in – we\u0026rsquo;re propagating the error back through the temporal structure of the network.\nThe gradients from each time step are summed to get the final gradient for each weight.\nFinally, we update the weights using an optimization algorithm like gradient descent.\nThe key challenge in BPTT is handling long sequences, as the gradients can become very small (vanishing gradient problem) or very large (exploding gradient problem) when propagated over many time steps.\nChallenges in Training RNNs While powerful, RNNs face some challenges during training:\nVanishing Gradients: As the network processes long sequences, gradients can become extremely small, making it difficult for the network to learn long-term dependencies.\nExploding Gradients: Conversely, gradients can also become extremely large, causing instability during training.\nLong-Term Dependencies: Standard RNNs often struggle to capture dependencies over long sequences.\nTo address these challenges, several variants of RNNs have been developed:\nLong Short-Term Memory (LSTM) Networks LSTMs introduce a more complex cell structure with gates to control the flow of information:\nForget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$ Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$ Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$ Cell state update: $C_t = f_t * C_{t-1} + i_t * \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$ Hidden state update: $h_t = o_t * \\tanh(C_t)$ Where $\\sigma$ is the sigmoid function and $*$ denotes element-wise multiplication.\nLSTMs are designed to overcome the vanishing gradient problem. They do this by introducing a new state called the cell state $C_t$, which acts as a conveyor belt of information flowing through the network. The LSTM can add or remove information from the cell state, carefully regulated by structures called gates. Let\u0026rsquo;s break down each component:\nForget gate ($f_t$): This gate decides what information to throw away from the cell state. It looks at $h_{t-1}$ and $x_t$, and outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$. A 1 represents \u0026ldquo;keep this\u0026rdquo; while a 0 represents \u0026ldquo;forget this.\u0026rdquo;\nInput gate ($i_t$): This gate decides which new information we\u0026rsquo;re going to store in the cell state. It has two parts:\nA sigmoid layer that decides which values we\u0026rsquo;ll update. A tanh layer that creates a vector of new candidate values that could be added to the state. Cell state update: We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t * \\tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.\nOutput gate ($o_t$): This gate decides what we\u0026rsquo;re going to output. This output will be based on our cell state, but will be a filtered version.\nHidden state update: Finally, we update the hidden state, which is a filtered version of the cell state.\nThe beauty of this architecture is that the cell state provides a direct avenue for information to flow through the network without being substantially changed, which helps with learning long-term dependencies.\nGated Recurrent Units (GRUs) GRUs simplify the LSTM architecture while maintaining many of its advantages:\nUpdate gate: $z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$ Reset gate: $r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$ Candidate hidden state: $\\tilde{h_t} = \\tanh(W \\cdot [r_t * h_{t-1}, x_t])$ Hidden state update: $h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$ GRUs are a simpler variant of LSTMs, combining the forget and input gates into a single \u0026ldquo;update gate\u0026rdquo;. They also merge the cell state and hidden state.\nUpdate gate ($z_t$): This gate decides how much of the past information (from previous time steps) needs to be passed along to the future. It can be thought of as the combination of the forget and input gates in an LSTM.\nReset gate ($r_t$): This gate is used to decide how much of the past information to forget.\nCandidate hidden state ($\\tilde{h}_t$): This is a new hidden state candidate, created using the reset gate to determine how much of the past state to incorporate.\nHidden state update: The final hidden state is a linear interpolation between the previous hidden state and the candidate hidden state, with the update gate determining the mix.\nThe main advantage of GRUs over LSTMs is that they\u0026rsquo;re computationally more efficient due to having fewer parameters. In practice, both GRUs and LSTMs tend to yield comparable results, with the best choice often depending on the specific dataset and task.\n","permalink":"https://adilsarsenov.dev/posts/recurrent-neural-networks/","summary":"Introduction Recurrent Neural Networks are a class of artificial neural networks designed to work with sequential data. Unlike feedforward neural networks, RNNs have loops in them, allowing information to persist. This makes them particularly suited for tasks where context and order matter, such as language translation, speech recognition, and time series prediction.\nThe Architecture of RNNs The basic structure of an RNN consists of a repeating module, often called a cell.","title":"Recurrent Neural Networks - RNN, LSTM, GRU"},{"content":"Introduction Generative Adversarial Networks (GANs) have emerged as a groundbreaking approach in generative modeling using deep learning techniques. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have revolutionized the way we generate synthetic data.\nWhat are GANs? Generative Adversarial Networks (GANs) designed to generate realistic data by training two neural networks, the generator and the discriminator, in a competitive setting. The generator creates synthetic data, while the discriminator evaluates the authenticity of the data, distinguishing between real and fake samples.\nComponents of GANs Generator: This network generates new data instances that resemble the training data. Discriminator: This network evaluates the authenticity of the data, distinguishing between real and generated instances. The generator aims to produce data that is indistinguishable from real data, while the discriminator tries to identify whether the data is real or generated. The two networks are trained together in a zero-sum game, improving each other iteratively.\nStructure of GANs GANs consist of two main parts:\nGenerative: The generator takes random noise as input and generates synthetic data. Adversarial: The discriminator tries to distinguish between real data and synthetic data produced by the generator. Mathematical Formulation The training of GANs involves the following objective functions for the generator and discriminator:\nDiscriminator Loss: $$ L_D = - \\mathbb{E}_{x \\sim p_d(x)}[\\log D(x)] - $$\n$$ \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))] $$\nGenerator Loss: $$ L_G = - \\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))] $$ where:\n( $D(x)$ ) is the discriminator\u0026rsquo;s estimate of the probability that real data instance ( $x$ ) is real. ( $G(z)$ ) is the generator\u0026rsquo;s output given noise ( $z$ ). ( $p_{d}(x)$ ) is the real data distribution. ( $p_z(z)$ ) is the prior distribution on input noise variables. Consider a simple GAN to generate handwritten digits similar to the MNIST dataset:\nGenerator Network: Takes random noise as input and generates a 28x28 image. Discriminator Network: Takes a 28x28 image as input and outputs a probability score indicating whether the image is real or fake. The networks are trained iteratively:\nTrain the discriminator on real and fake images. Train the generator to produce images that can fool the discriminator. CycleGAN CycleGANs are a variant of GANs designed for image-to-image translation without requiring paired examples. They are particularly useful for tasks where paired training data is unavailable.\nStructure of CycleGANs CycleGANs consist of two sets of generators and discriminators:\nGenerators: $( G: X \\to Y )$ and $( F: Y \\to X )$ Discriminators: $( D_Y )$ and $( D_X )$, which evaluate the realism of generated images. Loss Functions in CycleGANs Adversarial Loss: $$ L_{GAN}(G, D_Y, X, Y) = \\mathbb{E}_{y \\sim p_d(y)}[\\log (D_Y(y))] + $$\n$$ \\mathbb{E}_{x \\sim p_d(x)}[\\log (1 - D_Y(G(x)))] $$\nCycle-Consistency Loss: $$ L_{cyc}(G, F) = \\mathbb{E}_{x \\sim p_d(x)}[| F(G(x)) - x |_1] + $$\n$$ \\mathbb{E}_{y \\sim p_d(y)}[| G(F(y)) - y |_1] $$\nTotal Loss: $$ L(G, F, D_X, D_Y) = L_{GAN}(G, D_Y, X, Y) + L_{GAN}(F, D_X, Y, X) + \\lambda L_{cyc}(G, F) $$ where $( \\lambda )$ is a weight parameter that balances the two loss functions.\nExample Consider translating images from the domain of horses to the domain of zebras:\nGenerator $( G )$: Transforms horse images to zebra images. Generator $( F )$: Transforms zebra images to horse images. Discriminator $( D_Y )$: Evaluates whether an image is a real zebra or generated by $( G )$. Discriminator $( D_X )$: Evaluates whether an image is a real horse or generated by $( F )$. The training process involves:\nMinimizing the adversarial loss to ensure the generated images are indistinguishable from real images. Minimizing the cycle-consistency loss to ensure the content of the images is preserved during the translation. ","permalink":"https://adilsarsenov.dev/posts/gans/","summary":"Introduction Generative Adversarial Networks (GANs) have emerged as a groundbreaking approach in generative modeling using deep learning techniques. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have revolutionized the way we generate synthetic data.\nWhat are GANs? Generative Adversarial Networks (GANs) designed to generate realistic data by training two neural networks, the generator and the discriminator, in a competitive setting. The generator creates synthetic data, while the discriminator evaluates the authenticity of the data, distinguishing between real and fake samples.","title":"Generative Adversarial Networks (GANs) and CycleGANs"},{"content":"Introduction Evaluating model performance is as critical as designing and training the models themselves. Metrics provide quantifiable measures to assess the effectiveness and accuracy of models, guiding decisions for improvement and deployment.\nClassification Metrics Accuracy Accuracy is the most straightforward metric, representing the ratio of correctly predicted instances to the total instances.\n$$ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}} $$\nExample Calculation:\nTrue Positives (TP) = 50 True Negatives (TN) = 40 False Positives (FP) = 10 False Negatives (FN) = 0 Total Instances = 100 $$ \\text{Accuracy} = \\frac{50 + 40}{100} = \\frac{90}{100} = 0.90 , \\text{or} , 90\\text{%} $$\nPrecision, Recall, and F1 Score Precision and recall provide deeper insights, especially in imbalanced datasets.\nPrecision: The ratio of true positive predictions to the total predicted positives. $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$\nExample Calculation:\nTP = 30 FP = 10 $$ \\text{Precision} = \\frac{30}{30 + 10} = \\frac{30}{40} = 0.75 , \\text{or} , 75\\text{%} $$\nRecall: The ratio of true positive predictions to all actual positives. $$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\nExample Calculation:\nTP = 30 FN = 20 $$ \\text{Recall} = \\frac{30}{30 + 20} = \\frac{30}{50} = 0.60 , \\text{or} , 60\\text{%} $$\nF1 Score: The harmonic mean of precision and recall, balancing both metrics. $$ \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nExample Calculation:\nPrecision = 0.75 Recall = 0.60 $$ \\text{F1 Score} = 2 \\cdot \\frac{0.75 \\cdot 0.60}{0.75 + 0.60} = 2 \\cdot \\frac{0.45}{1.35} = \\frac{0.90}{1.35} = 0.67 , \\text{or} , 67\\text{%} $$\nSpecificity and Sensitivity Specificity: The ratio of true negative predictions to all actual negatives. It is used to measure the ability of the model to correctly identify negative instances. $$ \\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}} $$\nExample Calculation:\nTN = 70 FP = 10 $$ \\text{Specificity} = \\frac{70}{70 + 10} = \\frac{70}{80} = 0.875 , \\text{or} , 87.5\\text{%} $$\nSensitivity: Also known as recall, it measures the ability of the model to correctly identify positive instances. $$ \\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\nConfusion Matrix A confusion matrix is a table that provides a detailed breakdown of the performance of a classification model. It shows the number of true positive, true negative, false positive, and false negative predictions.\nExample Calculation:\nPredicted Positive Predicted Negative Actual Positive True Positive (TP) = 50 Type 2 Error: False Negative (FN) = 10 Actual Negative Type 1 Error: False Positive (FP) = 5 True Negative (TN) = 35 ROC-AUC The Receiver Operating Characteristic (ROC) curve is a graphical representation of a model\u0026rsquo;s ability to discriminate between positive and negative classes across different threshold values. The Area Under the Curve (AUC) quantifies this ability into a single scalar value.\nROC Curve: Plots the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings. AUC: The area under the ROC curve, where an AUC of 1 represents a perfect model, and an AUC of 0.5 represents a model with no discrimination capability. Example Calculation: Suppose we have the following TPR and FPR values at different thresholds:\nThreshold TPR (Recall) FPR (1 - Specificity) 0.1 0.95 0.50 0.2 0.90 0.30 0.3 0.85 0.20 0.4 0.80 0.15 0.5 0.75 0.10 Plotting these points on the ROC curve and calculating the area under this curve gives us the AUC.\nLog Loss (Cross-Entropy Loss) Log Loss evaluates the performance of a classification model where the prediction output is a probability value between 0 and 1.\n$$ \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $$\nExample Calculation:\nSuppose we have 3 instances with the following actual and predicted probabilities: Instance 1: $( y_1 = 1 ), ( \\hat{y}_1 = 0.9 )$ Instance 2: $( y_2 = 0 ), ( \\hat{y}_2 = 0.2 )$ Instance 3: $( y_3 = 1 ), ( \\hat{y}_3 = 0.7 )$ $$ \\text{Log Loss} = -\\frac{1}{3} [(1 \\cdot \\log(0.9) + (1 - 1) \\cdot \\log(1 - 0.9)) + (0 \\cdot \\log(0.2) + (1 - 0) \\cdot \\log(1 - 0.2)) + (1 \\cdot \\log(0.7) + (1 - 1) \\cdot \\log(1 - 0.7))] $$\n$$ \\text{Log Loss} = -\\frac{1}{3} [(-0.105) + (-0.223) + (-0.357)] = -\\frac{1}{3} [-0.685] = 0.228 $$\nMatthews Correlation Coefficient (MCC) MCC is a balanced measure that can be used even if the classes are of very different sizes. It considers true and false positives and negatives.\n$$ \\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} $$\nExample Calculation:\nTP = 50, TN = 40, FP = 10, FN = 10 $$ \\text{MCC} = \\frac{(50 \\cdot 40) - (10 \\cdot 10)}{\\sqrt{(50 + 10)(50 + 10)(40 + 10)(40 + 10)}} = \\frac{2000 - 100}{\\sqrt{60 \\cdot 60 \\cdot 50 \\cdot 50}} = \\frac{1900}{150000} = 0.63 $$\nRegression Metrics Mean Absolute Error (MAE) MAE measures the average magnitude of errors in predictions without considering their direction.\n$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\nExample Calculation:\nErrors = [1, 2, 3] $$ \\text{MAE} = \\frac{1 + 2 + 3}{3} = \\frac{6}{3} = 2 $$\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE) MSE squares the errors before averaging, penalizing larger errors more significantly.\n$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\nExample Calculation:\nErrors = [1, 2, 3] $$ \\text{MSE} = \\frac{1^2 + 2^2 + 3^2}{3} = \\frac{1 + 4 + 9}{3} = \\frac{14}{3} \\approx 4.67 $$\nRMSE is the square root of MSE, bringing the units back to the original scale.\n$$ \\text{RMSE} = \\sqrt{\\text{MSE}} $$\nExample Calculation:\nMSE = 4.67 $$ \\text{RMSE} = \\sqrt{4.67} \\approx 2.16 $$\nR-Squared (R²) R-Squared represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\nExample Calculation:\nTotal Sum of Squares (TSS) = 100 Residual Sum of Squares (RSS) = 20 $$ R^2 = 1 - \\frac{20}{100} = 1 - 0.20 = 0.80 , \\text{or} , 80\\text{%} $$\nMean Absolute Percentage Error (MAPE) MAPE measures the average absolute percentage error between predicted and actual values.\n$$ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $$\nExample Calculation:\nActual values = [100, 200, 300] Predicted values = [110, 190, 310] $$ \\text{MAPE} = \\frac{1}{3} \\left( \\left| \\frac{100 - 110}{100} \\right| + \\left| \\frac{200 - 190}{200} \\right| + \\left| \\frac{300 - 310}{300} \\right| \\right) = \\frac{1}{3} \\left(0.10 + 0.05 + 0.033\\right) = \\frac{1}{3} \\left(0.183\\right) \\approx 0.061 , \\text{or} , 6.1\\text{%} $$\nExplained Variance Score Explained variance measures how much of the variance in the target variable is explained by the model.\n$$ \\text{Explained Variance} = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)} $$\nExample Calculation:\nVariance of residuals = 20 Variance of target = 100 $$ \\text{Explained Variance} = 1 - \\frac{20}{100} = 1 - 0.20 = 0.80 , \\text{or} , 80\\text{%} $$\nComputer Vision Metrics Intersection over Union (IoU) IoU is crucial for segmentation and object detection tasks, measuring the overlap between the predicted and ground truth bounding boxes or segments.\n$$ \\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\nExample Calculation:\nArea of Overlap = 30 Area of Union = 50 $$ \\text{IoU} = \\frac{30}{50} = 0.60 , \\text{or} , 60\\text{%} $$\nMean Average Precision (mAP) mAP is commonly used in object detection, summarizing the precision-recall curve across multiple classes.\nCalculate the precision-recall curve for each class. Compute the Average Precision (AP) for each class. Take the mean of AP values across all classes. Example Calculation:\nAP values for three classes = [0.8, 0.7, 0.9] $$ \\text{mAP} = \\frac{0.8 + 0.7 + 0.9}{3} = \\frac{2.4}{3} = 0.80 , \\text{or} , 80\\text{%} $$\nDice Coefficient The Dice Coefficient, similar to IoU, is another metric for segmentation tasks, focusing on the overlap between predicted and ground truth segments.\n$$ \\text{Dice Coefficient} = \\frac{2 \\times \\text{Area of Overlap}}{\\text{Total Area of Predicted} + \\text{Total Area of Ground Truth}} $$\nExample Calculation:\nArea of Overlap = 30 Total Area of Predicted = 40 Total Area of Ground Truth = 50 $$ \\text{Dice Coefficient} = \\frac{2 \\times 30}{40 + 50} = \\frac{60}{90} = 0.67 , \\text{or} , 67\\text{%} $$\nPixel Accuracy Pixel Accuracy measures the proportion of correctly classified pixels in the entire image.\n$$ \\text{Pixel Accuracy} = \\frac{\\text{Number of Correct Pixels}}{\\text{Total Number of Pixels}} $$\nExample Calculation:\nNumber of Correct Pixels = 900 Total Number of Pixels = 1000 $$ \\text{Pixel Accuracy} = \\frac{900}{1000} = 0.90 , \\text{or} , 90\\text{%} $$\nMean IoU (mIoU) mIoU is the mean of the Intersection over Union (IoU) for all classes. It is commonly used for semantic segmentation tasks.\n$$ \\text{mIoU} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\nExample Calculation:\nIoU values for three classes = [0.6, 0.7, 0.8] $$ \\text{mIoU} = \\frac{0.6 + 0.7 + 0.8}{3} = \\frac{2.1}{3} = 0.70 , \\text{or} , 70\\text{%} $$\nStructural Similarity Index (SSIM) SSIM measures the similarity between two images, considering luminance, contrast, and structure.\n$$ \\text{SSIM}(x, y) = \\frac{(2\\mu_x \\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} $$\nExample Calculation: $$Assume ( \\mu_x = 100 ), ( \\mu_y = 105 ), ( \\sigma_x = 15 ), ( \\sigma_y = 20 ), ( \\sigma_{xy} = 18 ), ( C_1 = 6.5 ), ( C_2 = 58 )$$\n$$ \\text{SSIM} = \\frac{(2 \\cdot 100 \\cdot 105 + 6.5)(2 \\cdot 18 + 58)}{(100^2 + 105^2 + 6.5)(15^2 + 20^2 + 58)} $$\n$$ \\text{SSIM} = \\frac{(21000 + 6.5)(36 + 58)}{(10000 + 11025 + 6.5)(225 + 400 + 58)} $$\n$$ \\text{SSIM} = \\frac{21006.5 \\cdot 94}{21031.5 \\cdot 683} \\approx 0.42 $$\n","permalink":"https://adilsarsenov.dev/posts/metrics/","summary":"Introduction Evaluating model performance is as critical as designing and training the models themselves. Metrics provide quantifiable measures to assess the effectiveness and accuracy of models, guiding decisions for improvement and deployment.\nClassification Metrics Accuracy Accuracy is the most straightforward metric, representing the ratio of correctly predicted instances to the total instances.\n$$ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}} $$\nExample Calculation:\nTrue Positives (TP) = 50 True Negatives (TN) = 40 False Positives (FP) = 10 False Negatives (FN) = 0 Total Instances = 100 $$ \\text{Accuracy} = \\frac{50 + 40}{100} = \\frac{90}{100} = 0.","title":"Metrics in Machine Learning and Computer Vision"},{"content":"Introduction Image segmentation is a crucial technique in computer vision, enabling the division of an image into multiple meaningful and homogeneous regions or objects based on their inherent characteristics, such as color, texture, shape, or brightness. We delve into two powerful deep learning models for image segmentation: Fully Convolutional Networks (FCN) and U-Net.\nFully Convolutional Networks (FCN) Fully Convolutional Networks (FCNs) are a class of neural networks designed specifically for semantic segmentation. Unlike traditional Convolutional Neural Networks (CNNs) that produce a single label for the entire image, FCNs output a segmentation map where each pixel is classified into a particular category.\nArchitecture The architecture of an FCN is based on an encoder-decoder structure:\nEncoder (Downsampling Path): This part of the network extracts complex features from the input image through a series of convolutional and pooling layers. The spatial resolution is reduced while increasing the depth of the feature maps, allowing the network to capture high-level semantic information.\nDecoder (Upsampling Path): The decoder part of the network upscales the reduced-resolution feature maps back to the original image size. This is achieved through transposed convolution layers (also known as deconvolution layers), which learn the appropriate strides and padding to reconstruct the high-resolution segmentation map.\nKey Concepts Pooling (Downsampling): Pooling layers reduce the spatial resolution of the feature maps, which helps in capturing invariant features and reducing computational complexity. Common types include max pooling and average pooling. Unpooling (Upsampling): Pooling converts a patch of values to a single value, whereas unpooling does the opposite, converts a single value into a patch of values. Transposed Convolution (Upsampling): are used to increase the spatial resolution of the feature maps, essentially reversing the effect of pooling layers to reconstruct the detailed segmentation map. Skip Connections: One major issue with in-network downsampling in a FCN is that it reduces the resolution of the input by a large factor, thus during upsampling it becomes very difficult to reproduce the finer details even after using sophisticated techniques like Transpose Convolution. One-way to deal with this is by adding skip connections in the upsampling stage from earlier layers and summing the two feature maps. These connections transfer features from the encoder directly to the corresponding decoder layers, enabling the network to recover fine-grained details and produce more accurate segmentation boundaries.\nU-Net Overview U-Net is a specialized neural network architecture designed for biomedical image segmentation, introduced by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in 2015. Its distinctive U-shaped design, which features a symmetric encoder-decoder structure, has made it a popular choice in various medical image analysis tasks due to its impressive performance and efficiency.\nArchitecture The U-Net architecture consists of two main parts:\nEncoder (Contraction Path): Similar to FCN, the encoder part of U-Net captures high-level features through a series of convolutional and pooling layers. Each block typically consists of two 3x3 convolution layers followed by a ReLU activation function and a 2x2 max pooling layer.\nDecoder (Expansion Path): The decoder part upscales the feature maps to the original image size using transposed convolutions. At each upsampling step, the decoder concatenates the feature maps from the corresponding encoder layer via skip connections, providing rich contextual information for precise segmentation.\nKey Concepts Skip Connections: By concatenating the feature maps from the encoder to the decoder at each corresponding level, U-Net can leverage both high-level and low-level features, resulting in better localization and segmentation accuracy.\nData Augmentation: Given the often limited availability of annotated data in medical imaging, U-Net heavily relies on data augmentation techniques to enhance the diversity of the training dataset. This includes operations like rotations, flips, and elastic deformations.\n","permalink":"https://adilsarsenov.dev/posts/fcn-unet/","summary":"Introduction Image segmentation is a crucial technique in computer vision, enabling the division of an image into multiple meaningful and homogeneous regions or objects based on their inherent characteristics, such as color, texture, shape, or brightness. We delve into two powerful deep learning models for image segmentation: Fully Convolutional Networks (FCN) and U-Net.\nFully Convolutional Networks (FCN) Fully Convolutional Networks (FCNs) are a class of neural networks designed specifically for semantic segmentation.","title":"FCN and U-Net"},{"content":"Introduction Transfer learning (fine-tuning) is a powerful machine learning technique where a model developed for one task is reused as the starting point for a model on a second task.\nIt leverages the knowledge gained while solving one problem and applies it to a different but related problem. This method is particularly useful when the amount of data available for the new task is limited.\nThe main difference between fine-tuning and transfer learning is that, in the case of fine-tuning, the task is similar, while in the case of transfer learning, the task is different. However, in my opinion, this distinction is not always crucial, so I prefer to use these terms interchangeably.\nPre-trained Model Approach One common approach in transfer learning is to use a pre-trained model. Here\u0026rsquo;s how it works: Step 1: Select Source Model Choose a pre-trained model from available options. Many research institutions release models trained on large and challenging datasets, which can be used as the starting point.\nStep 2: Reuse Model The selected pre-trained model is then reused as the base for the new task. Depending on the specifics of the task, you might use the entire model or just parts of it.\nStep 3: Tune Model Finally, the model is fine-tuned on the new task\u0026rsquo;s data. This tuning process can involve adapting or refining the model based on the input-output pairs available for the new task.\nWhen to Use Transfer Learning? Transfer learning is particularly beneficial in the following scenarios:\nLimited Labeled Data: When there isn\u0026rsquo;t enough labeled training data to train a network from scratch. Similar Tasks: When there already exists a network pre-trained on a similar task, usually trained on massive amounts of data. Same Input: When the input for the new task is similar to the input for the pre-trained model. In general, the benefits of transfer learning may not be obvious until after the model has been developed and evaluated. However, it often enables the development of skillful models that would be challenging to create without it.\nImage Classification Transfer learning is widely used in image classification tasks. Pre-trained models like VGG, ResNet, and EfficientNet, which are trained on large datasets like ImageNet, are fine-tuned for specific image classification tasks with smaller datasets.\nObject Detection In object detection, transfer learning helps improve detection accuracy by leveraging pre-trained models. Frameworks like Faster R-CNN and YOLO often use pre-trained backbones to enhance feature extraction.\nNatural Language Processing (NLP) Transfer learning is also prevalent in NLP. Pre-trained language models like BERT, GPT, and T5 are fine-tuned for various NLP tasks such as sentiment analysis, translation, and question answering.\nMedical Image Analysis In medical imaging, transfer learning is used to detect anomalies in MRI scans, CT scans, and X-rays. Pre-trained models are fine-tuned to identify specific medical conditions, aiding in diagnosis and treatment planning.\n","permalink":"https://adilsarsenov.dev/posts/transfer-learning/","summary":"Introduction Transfer learning (fine-tuning) is a powerful machine learning technique where a model developed for one task is reused as the starting point for a model on a second task.\nIt leverages the knowledge gained while solving one problem and applies it to a different but related problem. This method is particularly useful when the amount of data available for the new task is limited.\nThe main difference between fine-tuning and transfer learning is that, in the case of fine-tuning, the task is similar, while in the case of transfer learning, the task is different.","title":"Understanding Transfer Learning"},{"content":"Introduction EfficientNet is an advanced deep learning model introduced by Mingxing Tan and Quoc V. Le from Google Research, Brain team, in their paper \u0026ldquo;EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u0026rdquo;. EfficientNet solves the common problem of balancing accuracy and resource consumption in deep learning models by using a novel technique called compound scaling.\nWhy EfficientNet? The Challenge of Traditional Models Traditional deep learning models often face a trade-off between accuracy and resource use. Making a model more accurate usually means making it larger, which requires more computational power and memory. EfficientNet tackles this challenge effectively.\nThe Solution: Compound Scaling EfficientNet introduces compound scaling, which scales three critical dimensions of a neural network: width, depth, and resolution. This scaling method ensures that the model is both efficient and accurate.\nWidth Scaling Width scaling refers to the number of channels(third dimension of an image) in each layer of the neural network. Increasing the width helps the model capture more complex patterns and features, leading to improved accuracy. Conversely, decreasing the width results in a more lightweight model suitable for environments with limited resources.\nDepth Scaling Depth scaling involves the total number of layers in the network. Deeper models can capture more intricate data representations but require more computational resources. Shallower models are computationally efficient but might sacrifice accuracy.\nResolution Scaling Resolution scaling adjusts the size of the input images. Higher-resolution images provide more detailed information, potentially improving performance. However, they also need more memory and computational power. Lower-resolution images consume fewer resources but may lose fine-grained details.\nMathematical Explanation of Compound Scaling EfficientNet uses a simple yet effective method to scale up models. The scaling method is guided by a compound coefficient $( \\phi )$ which uniformly scales network width, depth, and resolution:\n$$\n\\text{depth:} \\quad d = \\alpha^\\phi $$\n$$\n\\text{width:} \\quad w = \\beta^\\phi $$\n$$\n\\text{resolution:} \\quad r = \\gamma^\\phi $$\nwhere $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ are constants determined through a small grid search and $( \\phi )$ is a user-specified coefficient that controls how much to scale each dimension. The idea is to balance all three dimensions rather than scaling one aspect alone.\nGrid Search for EfficientNet The process of determining the constants $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ involves two main steps:\nStep 1: Baseline Network Assume twice the resources are available and set $( \\phi = 1 )$. Perform a small grid search for $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ based on equations 2 and 3 from the original paper. Specifically, the best values for EfficientNet-B0 are found to be $( \\alpha = 1.2 )$, $( \\beta = 1.1 )$, and $( \\gamma = 1.15 )$ under the constraint:\n$$\n\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 $$\nStep 2: Compound Scaling Fix $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ as constants and scale up the baseline network with different $( \\phi )$ values using the equation:\n$$\n\\text{New Depth} = \\alpha^\\phi \\times \\text{Baseline Depth} $$\n$$\n\\text{New Width} = \\beta^\\phi \\times \\text{Baseline Width} $$\n$$\n\\text{New Resolution} = \\gamma^\\phi \\times \\text{Baseline Resolution} $$\nThis method is used to obtain EfficientNet-B1 up to B7 models.\nEfficientNet Architecture EfficientNet uses Mobile Inverted Bottleneck (MBConv) layers, which combine depth-wise separable convolutions and inverted residual blocks. This architecture helps achieve high performance with fewer resources.\nInverted Residual Blocks(aka MBConv Block) Layers: EfficientNet uses these layers for efficient feature extraction. Compound Scaling: Scales width, depth, and resolution uniformly. Inside an Inverted Residual Block:\n1x1 Convolution (Expansion): Expands the number of channels. Depthwise Convolution: Applies lightweight filters to each channel independently. Squeeze-and-Excitation (SE): Assigns importance weights to each channel. 1x1 Convolution (Projection): Reduces the number of channels back to the original size. Residual Connection: Adds the original input back to the output. EfficientNet models have set new benchmarks for accuracy while being more resource-efficient than previous models. They are widely used for various computer vision tasks due to their balanced approach to scaling. Their architectures, such as EfficientNet-B0 up to EfficientNet-B7, provide a range of options depending on the required accuracy and available computational resources. ","permalink":"https://adilsarsenov.dev/posts/effnet/","summary":"Introduction EfficientNet is an advanced deep learning model introduced by Mingxing Tan and Quoc V. Le from Google Research, Brain team, in their paper \u0026ldquo;EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u0026rdquo;. EfficientNet solves the common problem of balancing accuracy and resource consumption in deep learning models by using a novel technique called compound scaling.\nWhy EfficientNet? The Challenge of Traditional Models Traditional deep learning models often face a trade-off between accuracy and resource use.","title":"EfficientNet - Compound Scaling"},{"content":"Introduction ResNet, which stands for Residual Network, is a revolutionary deep learning model created by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper \u0026ldquo;Deep Residual Learning for Image Recognition.\u0026rdquo; ResNet tackles big challenges in training very deep neural networks, like the vanishing gradient problem, and has achieved amazing results in various computer vision tasks.\nWhy ResNet? The Need for Deeper Networks Traditional Convolutional Neural Networks (CNNs) face performance issues when the network depth increases. This isn\u0026rsquo;t because of overfitting but rather an optimization challenge where adding more layers makes the network harder to train effectively.\nThe Vanishing Gradient Problem It has been found that there is a maximum threshold for depth with the traditional CNN. When neural networks get deeper, the gradients used to update weights during backpropagation become very small, leading to negligible updates. This is known as the vanishing gradient problem and it makes training very deep networks difficult.\nThe Solution: Residual Blocks ResNet introduces residual learning through residual blocks. Instead of expecting each layer to directly fit a desired mapping, residual blocks allow layers to fit a residual mapping. It\u0026rsquo;s easier to optimize the residual mapping than the original, unreferenced mapping.\nResidual Block Structure A typical residual block in ResNet has two or more convolutional layers followed by batch normalization and ReLU activation. The input to the block is added directly to the output of the stacked layers (this addition is the \u0026ldquo;shortcut connection\u0026rdquo;), creating a residual connection.\nMathematical Representation If the input is $ ( x ) $ and the desired output is $( H(x) ) $, a residual block models this as: $\\ H(x) = F(x, {W_i}) + x $ where $ ( F(x, {W_i}) ) $ represents the residual mapping to be learned.\nResNet Architecture ResNet architectures come in various depths, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152~201, indicating the number of layers. These architectures stack residual blocks to create deep networks that can effectively learn complex features.\nKey Components Convolutional Layers: Extract features from the input image. Batch Normalization: Normalizes the output of each convolutional layer, stabilizing and accelerating training. ReLU Activation: Introduces non-linearity. Residual Connections: Add the input of the block to the output, allowing the network to learn residual mappings. Achievements of ResNet ResNet won first place in the ILSVRC 2015 classification competition with a top-5 error rate of 3.57% using an ensemble model. It also won first place in the ImageNet Detection, ImageNet Localization, COCO Detection, and COCO Segmentation tasks in the ILSVRC and COCO 2015 competitions.\nNotably, replacing VGG-16 layers in Faster R-CNN with ResNet-101 layers led to a 28% relative improvement.\nTransfer Learning or Fine-tuning ResNet\u0026rsquo;s pre-trained models are often used for transfer learning, where a model trained on a large dataset like ImageNet is fine-tuned for specific tasks with smaller datasets.\nThis approach significantly reduces training time and improves performance.\n","permalink":"https://adilsarsenov.dev/posts/resnet/","summary":"Introduction ResNet, which stands for Residual Network, is a revolutionary deep learning model created by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper \u0026ldquo;Deep Residual Learning for Image Recognition.\u0026rdquo; ResNet tackles big challenges in training very deep neural networks, like the vanishing gradient problem, and has achieved amazing results in various computer vision tasks.\nWhy ResNet? The Need for Deeper Networks Traditional Convolutional Neural Networks (CNNs) face performance issues when the network depth increases.","title":"ResNet - Residual Blocks"},{"content":"Introduction The VGG (Visual Geometry Group) network is a renowned deep learning model known for its simplicity and effectiveness in image recognition tasks. Developed by K. Simonyan and A. Zisserman from Oxford University, the VGG network significantly advanced the field of computer vision and performed remarkably in the ILSVRC-2014 competition.\nVGG Architecture VGG networks are characterized by their deep architecture, which involves stacking multiple convolutional layers. The two most commonly used versions are VGG-16 and VGG-19, featuring 16 and 19 layers, respectively.\nKey Components of VGG Fixed Size Input: The network accepts a fixed size of (224 x 224) RGB images. Preprocessing: The only preprocessing step involves subtracting the mean RGB value from each pixel, computed over the entire training set (ImageNet). Kernel Size: VGG uses a small receptive field of 3x3 kernels with a stride of 1. Max-Pooling: Performed over a 2x2 pixel window with a stride of 2. Fully Connected Layers: VGG has three fully connected layers. The first two layers have 4096 neurons each, and the final layer has 1000 neurons, corresponding to the 1000 classes in the ImageNet dataset. Activation: Uses ReLU (Rectified Linear Unit) to introduce non-linearity. VGG-16 vs. VGG-19 The primary difference between VGG-16 and VGG-19 lies in the number of layers:\nVGG-16: Comprises 13 convolutional layers and 3 fully connected layers, making it slightly less complex and faster to train compared to VGG-19. VGG-19: Contains 16 convolutional layers and 3 fully connected layers, offering slightly better accuracy at the cost of increased computational resources and training time. Both models have demonstrated high accuracy in various benchmarks, but the choice between them depends on the specific application and the available computational resources.\nAchievements of VGG The VGG-16 model achieved a test accuracy of 92.7% on the ImageNet dataset, which includes over 14 million images across 1000 categories. This performance made it one of the top models in the ILSVRC-2014 competition.\n","permalink":"https://adilsarsenov.dev/posts/vgg/","summary":"Introduction The VGG (Visual Geometry Group) network is a renowned deep learning model known for its simplicity and effectiveness in image recognition tasks. Developed by K. Simonyan and A. Zisserman from Oxford University, the VGG network significantly advanced the field of computer vision and performed remarkably in the ILSVRC-2014 competition.\nVGG Architecture VGG networks are characterized by their deep architecture, which involves stacking multiple convolutional layers. The two most commonly used versions are VGG-16 and VGG-19, featuring 16 and 19 layers, respectively.","title":"Visual Geometry Group - VGG Architecture"},{"content":"Convolutional Neural Networks (CNNs) are a class of deep learning models designed to process visual data.\nTensors: A tensor can be conceptualized as an N-dimensional matrix.\nNeurons: A neuron functions as a computational unit, receiving multiple inputs and generating a single output.\nLayers: A layer consists of a collection of neurons that perform the same operation, sharing identical hyperparameters.\nKernel weights and biases: Unique to each neuron, kernel weights and biases are adjusted during the training process, enabling the classifier to adapt to the specific problem and dataset.\nKey Concepts of CNN Components Convolutional Layer The convolutional layer applies filter(kernel) to the input image to extract features like edges and textures.\nConvolutional Layer: The primary building block of a CNN, responsible for feature extraction. Filter (Kernel): A small matrix that slides over the input image, performing multiplications and summations to produce a feature map. Feature Map (Activation Map): The result of the convolution operation, highlighting important features such as edges, textures, and patterns. Convolution operation allows the network to learn spatial hierarchies of features automatically from low-level to high-level.\n$$ (I * K)(i, j) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} I(i + m, j + n) K(m, n) $$\n$( I )$: Input image $( K )$: Kernel (filter) $( (i, j) )$: Coordinates in the output feature map $( M, N )$: Dimensions of the kernel Understanding Hyperparameters Kernel Size: Dimensions of the filter (e.g., 3x3, 5x5). Affects the amount of detail the filter can capture. Stride: Step size of the filter movement. Larger strides reduce the output size but increase computational efficiency. Padding: Adds zeros around the input image to maintain the output size. \u0026ldquo;Valid\u0026rdquo; means no padding, \u0026ldquo;same\u0026rdquo; keeps the output size the same as the input. Non-Linearity (ReLU) Activation Function ReLU (Rectified Linear Unit): Introduces non-linearity to the model. It replaces negative values with zero, allowing the network to learn complex patterns. Activation function decides whether the neuron must be activated or not. So it means whether the neuron\u0026rsquo;s input is important to the network or not. $$ \\text{ReLU}(x) = \\begin{cases} x \u0026amp; \\text{if } x \u0026gt; 0 \\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$\nBy setting negative values to zero, ReLU prevents the network from simply becoming a linear classifier.\nAnother popular activation functions:\nPooling Layers Reduces computational complexity and helps the network become invariant to small translations of the input image.\nMax-Pooling: operation requires selecting a kernel size and a stride length. Once selected, the operation slides the kernel with the specified stride over the input retaining the most important information by selecting the maximum value within each window, effectively down-sampling the feature map. $$ Y(i, j) = \\max_{m,n} X(i \\cdot s + m, j \\cdot s + n) $$\n$( X )$: Input feature map $( Y )$: Output feature map $( s )$: Stride $( m, n )$: Window dimensions Pooling prevents overfitting.\nFlattening Flatten Layer: Converts the 2D pooled feature maps into a 1D vector for the fully connected layers. The flattened vector is fed as input to the fully connected layer to classify the image. Fully Connected Layer The Fully Connected Layer, also known as the dense layer, is the final layer that comes after the convolutional and pooling layers. Its purpose is to perform classification or regression tasks based on the high-level features extracted by the earlier layers of the network. In FC, all the neurons of the input are connected to every neuron of the output layer.\nFormula: $$ y = f(W \\cdot x + b) $$\n$( W )$: Weight matrix $( x )$: Input vector $( b )$: Bias vector $( f )$: Activation function The fully connected layer combines the high-level features learned by the convolutional layers to output a final prediction.\nOutput Layer The output layer is typically a softmax layer in classification tasks. The softmax function converts the raw output scores into probabilities.\nSoftmax Function: $$ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} $$\n$( z_i )$: The (i)-th element of the input vector $(z)$ $( K )$: Number of classes The softmax function ensures that the output probabilities sum to 1, making it easier to interpret the results as the likelihood of each class.\nSimple Example of CNN Architecture Here’s a simple CNN architecture for image classification:\nInput Layer: 28x28 grayscale image Convolutional Layer: conv layer with filter of the size 5x5, with valid padding(no padding), the feature maps have a size of 24x24xn1, where n1 is the number of filters used in this layer. Max-Pooling Layer: 2x2 window, stride 2 Convolutional Layer: conv layer with filter of the size 5x5, with valid padding(no padding) Max-Pooling Layer: 2x2 window, stride 2 Flatten Layer: Converts the feature maps 4x4 into a 1D vector with total size of 4x4xn2. Fully Connected Layer: the flattened vector is passed through a fully connected layer with n3 units, with ReLU activation function. Fully Connected Layer: passed again, applies Dropout to prevent overfitting Output Layer: Final Fully Connected Layer, which has 10 units corresponding to the 10 possible digit classes (0-9). This example illustrates the typical workflow in a CNN, from input to final classification.\n","permalink":"https://adilsarsenov.dev/posts/convolutional-neural-networks/","summary":"Convolutional Neural Networks (CNNs) are a class of deep learning models designed to process visual data.\nTensors: A tensor can be conceptualized as an N-dimensional matrix.\nNeurons: A neuron functions as a computational unit, receiving multiple inputs and generating a single output.\nLayers: A layer consists of a collection of neurons that perform the same operation, sharing identical hyperparameters.\nKernel weights and biases: Unique to each neuron, kernel weights and biases are adjusted during the training process, enabling the classifier to adapt to the specific problem and dataset.","title":"Convolutional Neural Networks - CNNs"},{"content":"","permalink":"https://adilsarsenov.dev/posts/leetcode-150-notes/","summary":"","title":""}]