[{"content":"Introduction Generative Adversarial Networks (GANs) have emerged as a groundbreaking approach in generative modeling using deep learning techniques. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have revolutionized the way we generate synthetic data.\nWhat are GANs? Generative Adversarial Networks (GANs) designed to generate realistic data by training two neural networks, the generator and the discriminator, in a competitive setting. The generator creates synthetic data, while the discriminator evaluates the authenticity of the data, distinguishing between real and fake samples.\nComponents of GANs Generator: This network generates new data instances that resemble the training data. Discriminator: This network evaluates the authenticity of the data, distinguishing between real and generated instances. The generator aims to produce data that is indistinguishable from real data, while the discriminator tries to identify whether the data is real or generated. The two networks are trained together in a zero-sum game, improving each other iteratively.\nStructure of GANs GANs consist of two main parts:\nGenerative: The generator takes random noise as input and generates synthetic data. Adversarial: The discriminator tries to distinguish between real data and synthetic data produced by the generator. Mathematical Formulation The training of GANs involves the following objective functions for the generator and discriminator:\nDiscriminator Loss: $$ L_D = - \\mathbb{E}_{x \\sim p_d(x)}[\\log D(x)] - $$\n$$ \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))] $$\nGenerator Loss: $$ L_G = - \\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))] $$ where:\n( $D(x)$ ) is the discriminator\u0026rsquo;s estimate of the probability that real data instance ( $x$ ) is real. ( $G(z)$ ) is the generator\u0026rsquo;s output given noise ( $z$ ). ( $p_{d}(x)$ ) is the real data distribution. ( $p_z(z)$ ) is the prior distribution on input noise variables. Consider a simple GAN to generate handwritten digits similar to the MNIST dataset:\nGenerator Network: Takes random noise as input and generates a 28x28 image. Discriminator Network: Takes a 28x28 image as input and outputs a probability score indicating whether the image is real or fake. The networks are trained iteratively:\nTrain the discriminator on real and fake images. Train the generator to produce images that can fool the discriminator. CycleGAN CycleGANs are a variant of GANs designed for image-to-image translation without requiring paired examples. They are particularly useful for tasks where paired training data is unavailable.\nStructure of CycleGANs CycleGANs consist of two sets of generators and discriminators:\nGenerators: $( G: X \\to Y )$ and $( F: Y \\to X )$ Discriminators: $( D_Y )$ and $( D_X )$, which evaluate the realism of generated images. Loss Functions in CycleGANs Adversarial Loss: $$ L_{GAN}(G, D_Y, X, Y) = \\mathbb{E}_{y \\sim p_d(y)}[\\log (D_Y(y))] + $$\n$$ \\mathbb{E}_{x \\sim p_d(x)}[\\log (1 - D_Y(G(x)))] $$\nCycle-Consistency Loss: $$ L_{cyc}(G, F) = \\mathbb{E}_{x \\sim p_d(x)}[| F(G(x)) - x |_1] + $$\n$$ \\mathbb{E}_{y \\sim p_d(y)}[| G(F(y)) - y |_1] $$\nTotal Loss: $$ L(G, F, D_X, D_Y) = L_{GAN}(G, D_Y, X, Y) + L_{GAN}(F, D_X, Y, X) + \\lambda L_{cyc}(G, F) $$ where $( \\lambda )$ is a weight parameter that balances the two loss functions.\nExample Consider translating images from the domain of horses to the domain of zebras:\nGenerator $( G )$: Transforms horse images to zebra images. Generator $( F )$: Transforms zebra images to horse images. Discriminator $( D_Y )$: Evaluates whether an image is a real zebra or generated by $( G )$. Discriminator $( D_X )$: Evaluates whether an image is a real horse or generated by $( F )$. The training process involves:\nMinimizing the adversarial loss to ensure the generated images are indistinguishable from real images. Minimizing the cycle-consistency loss to ensure the content of the images is preserved during the translation. ","permalink":"https://adilsarsenov.dev/posts/gans/","summary":"Introduction Generative Adversarial Networks (GANs) have emerged as a groundbreaking approach in generative modeling using deep learning techniques. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have revolutionized the way we generate synthetic data.\nWhat are GANs? Generative Adversarial Networks (GANs) designed to generate realistic data by training two neural networks, the generator and the discriminator, in a competitive setting. The generator creates synthetic data, while the discriminator evaluates the authenticity of the data, distinguishing between real and fake samples.","title":"Generative Adversarial Networks (GANs) and CycleGANs"},{"content":"Introduction Evaluating model performance is as critical as designing and training the models themselves. Metrics provide quantifiable measures to assess the effectiveness and accuracy of models, guiding decisions for improvement and deployment.\nClassification Metrics Accuracy Accuracy is the most straightforward metric, representing the ratio of correctly predicted instances to the total instances.\n$$ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}} $$\nExample Calculation:\nTrue Positives (TP) = 50 True Negatives (TN) = 40 False Positives (FP) = 10 False Negatives (FN) = 0 Total Instances = 100 $$ \\text{Accuracy} = \\frac{50 + 40}{100} = \\frac{90}{100} = 0.90 , \\text{or} , 90\\text{%} $$\nPrecision, Recall, and F1 Score Precision and recall provide deeper insights, especially in imbalanced datasets.\nPrecision: The ratio of true positive predictions to the total predicted positives. $$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} $$\nExample Calculation:\nTP = 30 FP = 10 $$ \\text{Precision} = \\frac{30}{30 + 10} = \\frac{30}{40} = 0.75 , \\text{or} , 75\\text{%} $$\nRecall: The ratio of true positive predictions to all actual positives. $$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\nExample Calculation:\nTP = 30 FN = 20 $$ \\text{Recall} = \\frac{30}{30 + 20} = \\frac{30}{50} = 0.60 , \\text{or} , 60\\text{%} $$\nF1 Score: The harmonic mean of precision and recall, balancing both metrics. $$ \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nExample Calculation:\nPrecision = 0.75 Recall = 0.60 $$ \\text{F1 Score} = 2 \\cdot \\frac{0.75 \\cdot 0.60}{0.75 + 0.60} = 2 \\cdot \\frac{0.45}{1.35} = \\frac{0.90}{1.35} = 0.67 , \\text{or} , 67\\text{%} $$\nSpecificity and Sensitivity Specificity: The ratio of true negative predictions to all actual negatives. It is used to measure the ability of the model to correctly identify negative instances. $$ \\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}} $$\nExample Calculation:\nTN = 70 FP = 10 $$ \\text{Specificity} = \\frac{70}{70 + 10} = \\frac{70}{80} = 0.875 , \\text{or} , 87.5\\text{%} $$\nSensitivity: Also known as recall, it measures the ability of the model to correctly identify positive instances. $$ \\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} $$\nConfusion Matrix A confusion matrix is a table that provides a detailed breakdown of the performance of a classification model. It shows the number of true positive, true negative, false positive, and false negative predictions.\nExample Calculation:\nPredicted Positive Predicted Negative Actual Positive True Positive (TP) = 50 Type 2 Error: False Negative (FN) = 10 Actual Negative Type 1 Error: False Positive (FP) = 5 True Negative (TN) = 35 ROC-AUC The Receiver Operating Characteristic (ROC) curve is a graphical representation of a model\u0026rsquo;s ability to discriminate between positive and negative classes across different threshold values. The Area Under the Curve (AUC) quantifies this ability into a single scalar value.\nROC Curve: Plots the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings. AUC: The area under the ROC curve, where an AUC of 1 represents a perfect model, and an AUC of 0.5 represents a model with no discrimination capability. Example Calculation: Suppose we have the following TPR and FPR values at different thresholds:\nThreshold TPR (Recall) FPR (1 - Specificity) 0.1 0.95 0.50 0.2 0.90 0.30 0.3 0.85 0.20 0.4 0.80 0.15 0.5 0.75 0.10 Plotting these points on the ROC curve and calculating the area under this curve gives us the AUC.\nLog Loss (Cross-Entropy Loss) Log Loss evaluates the performance of a classification model where the prediction output is a probability value between 0 and 1.\n$$ \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $$\nExample Calculation:\nSuppose we have 3 instances with the following actual and predicted probabilities: Instance 1: $( y_1 = 1 ), ( \\hat{y}_1 = 0.9 )$ Instance 2: $( y_2 = 0 ), ( \\hat{y}_2 = 0.2 )$ Instance 3: $( y_3 = 1 ), ( \\hat{y}_3 = 0.7 )$ $$ \\text{Log Loss} = -\\frac{1}{3} [(1 \\cdot \\log(0.9) + (1 - 1) \\cdot \\log(1 - 0.9)) + (0 \\cdot \\log(0.2) + (1 - 0) \\cdot \\log(1 - 0.2)) + (1 \\cdot \\log(0.7) + (1 - 1) \\cdot \\log(1 - 0.7))] $$\n$$ \\text{Log Loss} = -\\frac{1}{3} [(-0.105) + (-0.223) + (-0.357)] = -\\frac{1}{3} [-0.685] = 0.228 $$\nMatthews Correlation Coefficient (MCC) MCC is a balanced measure that can be used even if the classes are of very different sizes. It considers true and false positives and negatives.\n$$ \\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} $$\nExample Calculation:\nTP = 50, TN = 40, FP = 10, FN = 10 $$ \\text{MCC} = \\frac{(50 \\cdot 40) - (10 \\cdot 10)}{\\sqrt{(50 + 10)(50 + 10)(40 + 10)(40 + 10)}} = \\frac{2000 - 100}{\\sqrt{60 \\cdot 60 \\cdot 50 \\cdot 50}} = \\frac{1900}{150000} = 0.63 $$\nRegression Metrics Mean Absolute Error (MAE) MAE measures the average magnitude of errors in predictions without considering their direction.\n$$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\nExample Calculation:\nErrors = [1, 2, 3] $$ \\text{MAE} = \\frac{1 + 2 + 3}{3} = \\frac{6}{3} = 2 $$\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE) MSE squares the errors before averaging, penalizing larger errors more significantly.\n$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\nExample Calculation:\nErrors = [1, 2, 3] $$ \\text{MSE} = \\frac{1^2 + 2^2 + 3^2}{3} = \\frac{1 + 4 + 9}{3} = \\frac{14}{3} \\approx 4.67 $$\nRMSE is the square root of MSE, bringing the units back to the original scale.\n$$ \\text{RMSE} = \\sqrt{\\text{MSE}} $$\nExample Calculation:\nMSE = 4.67 $$ \\text{RMSE} = \\sqrt{4.67} \\approx 2.16 $$\nR-Squared (R²) R-Squared represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\nExample Calculation:\nTotal Sum of Squares (TSS) = 100 Residual Sum of Squares (RSS) = 20 $$ R^2 = 1 - \\frac{20}{100} = 1 - 0.20 = 0.80 , \\text{or} , 80\\text{%} $$\nMean Absolute Percentage Error (MAPE) MAPE measures the average absolute percentage error between predicted and actual values.\n$$ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $$\nExample Calculation:\nActual values = [100, 200, 300] Predicted values = [110, 190, 310] $$ \\text{MAPE} = \\frac{1}{3} \\left( \\left| \\frac{100 - 110}{100} \\right| + \\left| \\frac{200 - 190}{200} \\right| + \\left| \\frac{300 - 310}{300} \\right| \\right) = \\frac{1}{3} \\left(0.10 + 0.05 + 0.033\\right) = \\frac{1}{3} \\left(0.183\\right) \\approx 0.061 , \\text{or} , 6.1\\text{%} $$\nExplained Variance Score Explained variance measures how much of the variance in the target variable is explained by the model.\n$$ \\text{Explained Variance} = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)} $$\nExample Calculation:\nVariance of residuals = 20 Variance of target = 100 $$ \\text{Explained Variance} = 1 - \\frac{20}{100} = 1 - 0.20 = 0.80 , \\text{or} , 80\\text{%} $$\nComputer Vision Metrics Intersection over Union (IoU) IoU is crucial for segmentation and object detection tasks, measuring the overlap between the predicted and ground truth bounding boxes or segments.\n$$ \\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\nExample Calculation:\nArea of Overlap = 30 Area of Union = 50 $$ \\text{IoU} = \\frac{30}{50} = 0.60 , \\text{or} , 60\\text{%} $$\nMean Average Precision (mAP) mAP is commonly used in object detection, summarizing the precision-recall curve across multiple classes.\nCalculate the precision-recall curve for each class. Compute the Average Precision (AP) for each class. Take the mean of AP values across all classes. Example Calculation:\nAP values for three classes = [0.8, 0.7, 0.9] $$ \\text{mAP} = \\frac{0.8 + 0.7 + 0.9}{3} = \\frac{2.4}{3} = 0.80 , \\text{or} , 80\\text{%} $$\nDice Coefficient The Dice Coefficient, similar to IoU, is another metric for segmentation tasks, focusing on the overlap between predicted and ground truth segments.\n$$ \\text{Dice Coefficient} = \\frac{2 \\times \\text{Area of Overlap}}{\\text{Total Area of Predicted} + \\text{Total Area of Ground Truth}} $$\nExample Calculation:\nArea of Overlap = 30 Total Area of Predicted = 40 Total Area of Ground Truth = 50 $$ \\text{Dice Coefficient} = \\frac{2 \\times 30}{40 + 50} = \\frac{60}{90} = 0.67 , \\text{or} , 67\\text{%} $$\nPixel Accuracy Pixel Accuracy measures the proportion of correctly classified pixels in the entire image.\n$$ \\text{Pixel Accuracy} = \\frac{\\text{Number of Correct Pixels}}{\\text{Total Number of Pixels}} $$\nExample Calculation:\nNumber of Correct Pixels = 900 Total Number of Pixels = 1000 $$ \\text{Pixel Accuracy} = \\frac{900}{1000} = 0.90 , \\text{or} , 90\\text{%} $$\nMean IoU (mIoU) mIoU is the mean of the Intersection over Union (IoU) for all classes. It is commonly used for semantic segmentation tasks.\n$$ \\text{mIoU} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\nExample Calculation:\nIoU values for three classes = [0.6, 0.7, 0.8] $$ \\text{mIoU} = \\frac{0.6 + 0.7 + 0.8}{3} = \\frac{2.1}{3} = 0.70 , \\text{or} , 70\\text{%} $$\nStructural Similarity Index (SSIM) SSIM measures the similarity between two images, considering luminance, contrast, and structure.\n$$ \\text{SSIM}(x, y) = \\frac{(2\\mu_x \\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)} $$\nExample Calculation: $$Assume ( \\mu_x = 100 ), ( \\mu_y = 105 ), ( \\sigma_x = 15 ), ( \\sigma_y = 20 ), ( \\sigma_{xy} = 18 ), ( C_1 = 6.5 ), ( C_2 = 58 )$$\n$$ \\text{SSIM} = \\frac{(2 \\cdot 100 \\cdot 105 + 6.5)(2 \\cdot 18 + 58)}{(100^2 + 105^2 + 6.5)(15^2 + 20^2 + 58)} $$\n$$ \\text{SSIM} = \\frac{(21000 + 6.5)(36 + 58)}{(10000 + 11025 + 6.5)(225 + 400 + 58)} $$\n$$ \\text{SSIM} = \\frac{21006.5 \\cdot 94}{21031.5 \\cdot 683} \\approx 0.42 $$\n","permalink":"https://adilsarsenov.dev/posts/metrics/","summary":"Introduction Evaluating model performance is as critical as designing and training the models themselves. Metrics provide quantifiable measures to assess the effectiveness and accuracy of models, guiding decisions for improvement and deployment.\nClassification Metrics Accuracy Accuracy is the most straightforward metric, representing the ratio of correctly predicted instances to the total instances.\n$$ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}} $$\nExample Calculation:\nTrue Positives (TP) = 50 True Negatives (TN) = 40 False Positives (FP) = 10 False Negatives (FN) = 0 Total Instances = 100 $$ \\text{Accuracy} = \\frac{50 + 40}{100} = \\frac{90}{100} = 0.","title":"Metrics in Machine Learning and Computer Vision"},{"content":"Introduction Image segmentation is a crucial technique in computer vision, enabling the division of an image into multiple meaningful and homogeneous regions or objects based on their inherent characteristics, such as color, texture, shape, or brightness. We delve into two powerful deep learning models for image segmentation: Fully Convolutional Networks (FCN) and U-Net.\nFully Convolutional Networks (FCN) Fully Convolutional Networks (FCNs) are a class of neural networks designed specifically for semantic segmentation. Unlike traditional Convolutional Neural Networks (CNNs) that produce a single label for the entire image, FCNs output a segmentation map where each pixel is classified into a particular category.\nArchitecture The architecture of an FCN is based on an encoder-decoder structure:\nEncoder (Downsampling Path): This part of the network extracts complex features from the input image through a series of convolutional and pooling layers. The spatial resolution is reduced while increasing the depth of the feature maps, allowing the network to capture high-level semantic information.\nDecoder (Upsampling Path): The decoder part of the network upscales the reduced-resolution feature maps back to the original image size. This is achieved through transposed convolution layers (also known as deconvolution layers), which learn the appropriate strides and padding to reconstruct the high-resolution segmentation map.\nKey Concepts Pooling (Downsampling): Pooling layers reduce the spatial resolution of the feature maps, which helps in capturing invariant features and reducing computational complexity. Common types include max pooling and average pooling. Unpooling (Upsampling): Pooling converts a patch of values to a single value, whereas unpooling does the opposite, converts a single value into a patch of values. Transposed Convolution (Upsampling): are used to increase the spatial resolution of the feature maps, essentially reversing the effect of pooling layers to reconstruct the detailed segmentation map. Skip Connections: One major issue with in-network downsampling in a FCN is that it reduces the resolution of the input by a large factor, thus during upsampling it becomes very difficult to reproduce the finer details even after using sophisticated techniques like Transpose Convolution. One-way to deal with this is by adding skip connections in the upsampling stage from earlier layers and summing the two feature maps. These connections transfer features from the encoder directly to the corresponding decoder layers, enabling the network to recover fine-grained details and produce more accurate segmentation boundaries.\nU-Net Overview U-Net is a specialized neural network architecture designed for biomedical image segmentation, introduced by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in 2015. Its distinctive U-shaped design, which features a symmetric encoder-decoder structure, has made it a popular choice in various medical image analysis tasks due to its impressive performance and efficiency.\nArchitecture The U-Net architecture consists of two main parts:\nEncoder (Contraction Path): Similar to FCN, the encoder part of U-Net captures high-level features through a series of convolutional and pooling layers. Each block typically consists of two 3x3 convolution layers followed by a ReLU activation function and a 2x2 max pooling layer.\nDecoder (Expansion Path): The decoder part upscales the feature maps to the original image size using transposed convolutions. At each upsampling step, the decoder concatenates the feature maps from the corresponding encoder layer via skip connections, providing rich contextual information for precise segmentation.\nKey Concepts Skip Connections: By concatenating the feature maps from the encoder to the decoder at each corresponding level, U-Net can leverage both high-level and low-level features, resulting in better localization and segmentation accuracy.\nData Augmentation: Given the often limited availability of annotated data in medical imaging, U-Net heavily relies on data augmentation techniques to enhance the diversity of the training dataset. This includes operations like rotations, flips, and elastic deformations.\n","permalink":"https://adilsarsenov.dev/posts/fcn_unet/","summary":"Introduction Image segmentation is a crucial technique in computer vision, enabling the division of an image into multiple meaningful and homogeneous regions or objects based on their inherent characteristics, such as color, texture, shape, or brightness. We delve into two powerful deep learning models for image segmentation: Fully Convolutional Networks (FCN) and U-Net.\nFully Convolutional Networks (FCN) Fully Convolutional Networks (FCNs) are a class of neural networks designed specifically for semantic segmentation.","title":"FCN and U-Net"},{"content":"Introduction Transfer learning (fine-tuning) is a powerful machine learning technique where a model developed for one task is reused as the starting point for a model on a second task.\nIt leverages the knowledge gained while solving one problem and applies it to a different but related problem. This method is particularly useful when the amount of data available for the new task is limited.\nThe main difference between fine-tuning and transfer learning is that, in the case of fine-tuning, the task is similar, while in the case of transfer learning, the task is different. However, in my opinion, this distinction is not always crucial, so I prefer to use these terms interchangeably.\nPre-trained Model Approach One common approach in transfer learning is to use a pre-trained model. Here\u0026rsquo;s how it works: Step 1: Select Source Model Choose a pre-trained model from available options. Many research institutions release models trained on large and challenging datasets, which can be used as the starting point.\nStep 2: Reuse Model The selected pre-trained model is then reused as the base for the new task. Depending on the specifics of the task, you might use the entire model or just parts of it.\nStep 3: Tune Model Finally, the model is fine-tuned on the new task\u0026rsquo;s data. This tuning process can involve adapting or refining the model based on the input-output pairs available for the new task.\nWhen to Use Transfer Learning? Transfer learning is particularly beneficial in the following scenarios:\nLimited Labeled Data: When there isn\u0026rsquo;t enough labeled training data to train a network from scratch. Similar Tasks: When there already exists a network pre-trained on a similar task, usually trained on massive amounts of data. Same Input: When the input for the new task is similar to the input for the pre-trained model. In general, the benefits of transfer learning may not be obvious until after the model has been developed and evaluated. However, it often enables the development of skillful models that would be challenging to create without it.\nImage Classification Transfer learning is widely used in image classification tasks. Pre-trained models like VGG, ResNet, and EfficientNet, which are trained on large datasets like ImageNet, are fine-tuned for specific image classification tasks with smaller datasets.\nObject Detection In object detection, transfer learning helps improve detection accuracy by leveraging pre-trained models. Frameworks like Faster R-CNN and YOLO often use pre-trained backbones to enhance feature extraction.\nNatural Language Processing (NLP) Transfer learning is also prevalent in NLP. Pre-trained language models like BERT, GPT, and T5 are fine-tuned for various NLP tasks such as sentiment analysis, translation, and question answering.\nMedical Image Analysis In medical imaging, transfer learning is used to detect anomalies in MRI scans, CT scans, and X-rays. Pre-trained models are fine-tuned to identify specific medical conditions, aiding in diagnosis and treatment planning.\n","permalink":"https://adilsarsenov.dev/posts/transfer-learning/","summary":"Introduction Transfer learning (fine-tuning) is a powerful machine learning technique where a model developed for one task is reused as the starting point for a model on a second task.\nIt leverages the knowledge gained while solving one problem and applies it to a different but related problem. This method is particularly useful when the amount of data available for the new task is limited.\nThe main difference between fine-tuning and transfer learning is that, in the case of fine-tuning, the task is similar, while in the case of transfer learning, the task is different.","title":"Understanding Transfer Learning"},{"content":"Introduction EfficientNet is an advanced deep learning model introduced by Mingxing Tan and Quoc V. Le from Google Research, Brain team, in their paper \u0026ldquo;EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u0026rdquo;. EfficientNet solves the common problem of balancing accuracy and resource consumption in deep learning models by using a novel technique called compound scaling.\nWhy EfficientNet? The Challenge of Traditional Models Traditional deep learning models often face a trade-off between accuracy and resource use. Making a model more accurate usually means making it larger, which requires more computational power and memory. EfficientNet tackles this challenge effectively.\nThe Solution: Compound Scaling EfficientNet introduces compound scaling, which scales three critical dimensions of a neural network: width, depth, and resolution. This scaling method ensures that the model is both efficient and accurate.\nWidth Scaling Width scaling refers to the number of channels(third dimension of an image) in each layer of the neural network. Increasing the width helps the model capture more complex patterns and features, leading to improved accuracy. Conversely, decreasing the width results in a more lightweight model suitable for environments with limited resources.\nDepth Scaling Depth scaling involves the total number of layers in the network. Deeper models can capture more intricate data representations but require more computational resources. Shallower models are computationally efficient but might sacrifice accuracy.\nResolution Scaling Resolution scaling adjusts the size of the input images. Higher-resolution images provide more detailed information, potentially improving performance. However, they also need more memory and computational power. Lower-resolution images consume fewer resources but may lose fine-grained details.\nMathematical Explanation of Compound Scaling EfficientNet uses a simple yet effective method to scale up models. The scaling method is guided by a compound coefficient $( \\phi )$ which uniformly scales network width, depth, and resolution:\n$$\n\\text{depth:} \\quad d = \\alpha^\\phi $$\n$$\n\\text{width:} \\quad w = \\beta^\\phi $$\n$$\n\\text{resolution:} \\quad r = \\gamma^\\phi $$\nwhere $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ are constants determined through a small grid search and $( \\phi )$ is a user-specified coefficient that controls how much to scale each dimension. The idea is to balance all three dimensions rather than scaling one aspect alone.\nGrid Search for EfficientNet The process of determining the constants $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ involves two main steps:\nStep 1: Baseline Network Assume twice the resources are available and set $( \\phi = 1 )$. Perform a small grid search for $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ based on equations 2 and 3 from the original paper. Specifically, the best values for EfficientNet-B0 are found to be $( \\alpha = 1.2 )$, $( \\beta = 1.1 )$, and $( \\gamma = 1.15 )$ under the constraint:\n$$\n\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 $$\nStep 2: Compound Scaling Fix $( \\alpha )$, $( \\beta )$, and $( \\gamma )$ as constants and scale up the baseline network with different $( \\phi )$ values using the equation:\n$$\n\\text{New Depth} = \\alpha^\\phi \\times \\text{Baseline Depth} $$\n$$\n\\text{New Width} = \\beta^\\phi \\times \\text{Baseline Width} $$\n$$\n\\text{New Resolution} = \\gamma^\\phi \\times \\text{Baseline Resolution} $$\nThis method is used to obtain EfficientNet-B1 up to B7 models.\nEfficientNet Architecture EfficientNet uses Mobile Inverted Bottleneck (MBConv) layers, which combine depth-wise separable convolutions and inverted residual blocks. This architecture helps achieve high performance with fewer resources.\nInverted Residual Blocks(aka MBConv Block) Layers: EfficientNet uses these layers for efficient feature extraction. Compound Scaling: Scales width, depth, and resolution uniformly. Inside an Inverted Residual Block:\n1x1 Convolution (Expansion): Expands the number of channels. Depthwise Convolution: Applies lightweight filters to each channel independently. Squeeze-and-Excitation (SE): Assigns importance weights to each channel. 1x1 Convolution (Projection): Reduces the number of channels back to the original size. Residual Connection: Adds the original input back to the output. EfficientNet models have set new benchmarks for accuracy while being more resource-efficient than previous models. They are widely used for various computer vision tasks due to their balanced approach to scaling. Their architectures, such as EfficientNet-B0 up to EfficientNet-B7, provide a range of options depending on the required accuracy and available computational resources. ","permalink":"https://adilsarsenov.dev/posts/effnet/","summary":"Introduction EfficientNet is an advanced deep learning model introduced by Mingxing Tan and Quoc V. Le from Google Research, Brain team, in their paper \u0026ldquo;EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u0026rdquo;. EfficientNet solves the common problem of balancing accuracy and resource consumption in deep learning models by using a novel technique called compound scaling.\nWhy EfficientNet? The Challenge of Traditional Models Traditional deep learning models often face a trade-off between accuracy and resource use.","title":"EfficientNet - Compound Scaling"},{"content":"Introduction ResNet, which stands for Residual Network, is a revolutionary deep learning model created by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper \u0026ldquo;Deep Residual Learning for Image Recognition.\u0026rdquo; ResNet tackles big challenges in training very deep neural networks, like the vanishing gradient problem, and has achieved amazing results in various computer vision tasks.\nWhy ResNet? The Need for Deeper Networks Traditional Convolutional Neural Networks (CNNs) face performance issues when the network depth increases. This isn\u0026rsquo;t because of overfitting but rather an optimization challenge where adding more layers makes the network harder to train effectively.\nThe Vanishing Gradient Problem It has been found that there is a maximum threshold for depth with the traditional CNN. When neural networks get deeper, the gradients used to update weights during backpropagation become very small, leading to negligible updates. This is known as the vanishing gradient problem and it makes training very deep networks difficult.\nThe Solution: Residual Blocks ResNet introduces residual learning through residual blocks. Instead of expecting each layer to directly fit a desired mapping, residual blocks allow layers to fit a residual mapping. It\u0026rsquo;s easier to optimize the residual mapping than the original, unreferenced mapping.\nResidual Block Structure A typical residual block in ResNet has two or more convolutional layers followed by batch normalization and ReLU activation. The input to the block is added directly to the output of the stacked layers (this addition is the \u0026ldquo;shortcut connection\u0026rdquo;), creating a residual connection.\nMathematical Representation If the input is $ ( x ) $ and the desired output is $( H(x) ) $, a residual block models this as: $\\ H(x) = F(x, {W_i}) + x $ where $ ( F(x, {W_i}) ) $ represents the residual mapping to be learned.\nResNet Architecture ResNet architectures come in various depths, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152~201, indicating the number of layers. These architectures stack residual blocks to create deep networks that can effectively learn complex features.\nKey Components Convolutional Layers: Extract features from the input image. Batch Normalization: Normalizes the output of each convolutional layer, stabilizing and accelerating training. ReLU Activation: Introduces non-linearity. Residual Connections: Add the input of the block to the output, allowing the network to learn residual mappings. Achievements of ResNet ResNet won first place in the ILSVRC 2015 classification competition with a top-5 error rate of 3.57% using an ensemble model. It also won first place in the ImageNet Detection, ImageNet Localization, COCO Detection, and COCO Segmentation tasks in the ILSVRC and COCO 2015 competitions.\nNotably, replacing VGG-16 layers in Faster R-CNN with ResNet-101 layers led to a 28% relative improvement.\nTransfer Learning or Fine-tuning ResNet\u0026rsquo;s pre-trained models are often used for transfer learning, where a model trained on a large dataset like ImageNet is fine-tuned for specific tasks with smaller datasets.\nThis approach significantly reduces training time and improves performance.\n","permalink":"https://adilsarsenov.dev/posts/resnet/","summary":"Introduction ResNet, which stands for Residual Network, is a revolutionary deep learning model created by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper \u0026ldquo;Deep Residual Learning for Image Recognition.\u0026rdquo; ResNet tackles big challenges in training very deep neural networks, like the vanishing gradient problem, and has achieved amazing results in various computer vision tasks.\nWhy ResNet? The Need for Deeper Networks Traditional Convolutional Neural Networks (CNNs) face performance issues when the network depth increases.","title":"ResNet - Residual Blocks"},{"content":"Introduction The VGG (Visual Geometry Group) network is a renowned deep learning model known for its simplicity and effectiveness in image recognition tasks. Developed by K. Simonyan and A. Zisserman from Oxford University, the VGG network significantly advanced the field of computer vision and performed remarkably in the ILSVRC-2014 competition.\nVGG Architecture VGG networks are characterized by their deep architecture, which involves stacking multiple convolutional layers. The two most commonly used versions are VGG-16 and VGG-19, featuring 16 and 19 layers, respectively.\nKey Components of VGG Fixed Size Input: The network accepts a fixed size of (224 x 224) RGB images. Preprocessing: The only preprocessing step involves subtracting the mean RGB value from each pixel, computed over the entire training set (ImageNet). Kernel Size: VGG uses a small receptive field of 3x3 kernels with a stride of 1. Max-Pooling: Performed over a 2x2 pixel window with a stride of 2. Fully Connected Layers: VGG has three fully connected layers. The first two layers have 4096 neurons each, and the final layer has 1000 neurons, corresponding to the 1000 classes in the ImageNet dataset. Activation: Uses ReLU (Rectified Linear Unit) to introduce non-linearity. VGG-16 vs. VGG-19 The primary difference between VGG-16 and VGG-19 lies in the number of layers:\nVGG-16: Comprises 13 convolutional layers and 3 fully connected layers, making it slightly less complex and faster to train compared to VGG-19. VGG-19: Contains 16 convolutional layers and 3 fully connected layers, offering slightly better accuracy at the cost of increased computational resources and training time. Both models have demonstrated high accuracy in various benchmarks, but the choice between them depends on the specific application and the available computational resources.\nAchievements of VGG The VGG-16 model achieved a test accuracy of 92.7% on the ImageNet dataset, which includes over 14 million images across 1000 categories. This performance made it one of the top models in the ILSVRC-2014 competition.\n","permalink":"https://adilsarsenov.dev/posts/vgg/","summary":"Introduction The VGG (Visual Geometry Group) network is a renowned deep learning model known for its simplicity and effectiveness in image recognition tasks. Developed by K. Simonyan and A. Zisserman from Oxford University, the VGG network significantly advanced the field of computer vision and performed remarkably in the ILSVRC-2014 competition.\nVGG Architecture VGG networks are characterized by their deep architecture, which involves stacking multiple convolutional layers. The two most commonly used versions are VGG-16 and VGG-19, featuring 16 and 19 layers, respectively.","title":"Visual Geometry Group - VGG Architecture"},{"content":"Convolutional Neural Networks (CNNs) are a class of deep learning models designed to process visual data. CNNs mimic the way the human brain processes visual information, making them incredibly powerful for visual tasks.\nKey Concepts of CNN Components Convolutional Layer The convolutional layer applies filter(kernel) to the input image to extract features like edges and textures.\nConvolutional Layer: The primary building block of a CNN, responsible for feature extraction. Filter (Kernel): A small matrix that slides over the input image, performing multiplications and summations to produce a feature map. Feature Map (Activation Map): The result of the convolution operation, highlighting important features such as edges, textures, and patterns. Convolution operation allows the network to learn spatial hierarchies of features automatically from low-level to high-level.\n$$ (I * K)(i, j) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} I(i + m, j + n) K(m, n) $$\n$( I )$: Input image $( K )$: Kernel (filter) $( (i, j) )$: Coordinates in the output feature map $( M, N )$: Dimensions of the kernel Understanding Hyperparameters Kernel Size: Dimensions of the filter (e.g., 3x3, 5x5). Affects the amount of detail the filter can capture. Stride: Step size of the filter movement. Larger strides reduce the output size but increase computational efficiency. Padding: Adds zeros around the input image to maintain the output size. \u0026ldquo;Valid\u0026rdquo; means no padding, \u0026ldquo;same\u0026rdquo; keeps the output size the same as the input. Non-Linearity (ReLU) Activation Function ReLU (Rectified Linear Unit): Introduces non-linearity to the model. It replaces negative values with zero, allowing the network to learn complex patterns. Activation function decides whether the neuron must be activated or not. So it means whether the neuron\u0026rsquo;s input is important to the network or not. $$ \\text{ReLU}(x) = \\begin{cases} x \u0026amp; \\text{if } x \u0026gt; 0 \\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$\nBy setting negative values to zero, ReLU prevents the network from simply becoming a linear classifier.\nAnother popular activation functions:\nPooling Layers Reduces computational complexity and helps the network become invariant to small translations of the input image.\nMax-Pooling: operation requires selecting a kernel size and a stride length. Once selected, the operation slides the kernel with the specified stride over the input retaining the most important information by selecting the maximum value within each window, effectively down-sampling the feature map. $$ Y(i, j) = \\max_{m,n} X(i \\cdot s + m, j \\cdot s + n) $$\n$( X )$: Input feature map $( Y )$: Output feature map $( s )$: Stride $( m, n )$: Window dimensions Pooling prevents overfitting.\nFlattening Flatten Layer: Converts the 2D pooled feature maps into a 1D vector for the fully connected layers. The flattened vector is fed as input to the fully connected layer to classify the image. Fully Connected Layer The Fully Connected Layer, also known as the dense layer, is the final layer that comes after the convolutional and pooling layers. Its purpose is to perform classification or regression tasks based on the high-level features extracted by the earlier layers of the network. In FC, all the neurons of the input are connected to every neuron of the output layer.\nFormula: $$ y = f(W \\cdot x + b) $$\n$( W )$: Weight matrix $( x )$: Input vector $( b )$: Bias vector $( f )$: Activation function The fully connected layer combines the high-level features learned by the convolutional layers to output a final prediction.\nOutput Layer The output layer is typically a softmax layer in classification tasks. The softmax function converts the raw output scores into probabilities.\nSoftmax Function: $$ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} $$\n$( z_i )$: The (i)-th element of the input vector $(z)$ $( K )$: Number of classes The softmax function ensures that the output probabilities sum to 1, making it easier to interpret the results as the likelihood of each class.\nSimple Example of CNN Architecture Here’s a simple CNN architecture for image classification:\nInput Layer: 28x28 grayscale image Convolutional Layer: conv layer with filter of the size 5x5, with valid padding(no padding), the feature maps have a size of 24x24xn1, where n1 is the number of filters used in this layer. Max-Pooling Layer: 2x2 window, stride 2 Convolutional Layer: conv layer with filter of the size 5x5, with valid padding(no padding) Max-Pooling Layer: 2x2 window, stride 2 Flatten Layer: Converts the feature maps 4x4 into a 1D vector with total size of 4x4xn2. Fully Connected Layer: the flattened vector is passed through a fully connected layer with n3 units, with ReLU activation function. Fully Connected Layer: passed again, applies Dropout to prevent overfitting Output Layer: Final Fully Connected Layer, which has 10 units corresponding to the 10 possible digit classes (0-9). This example illustrates the typical workflow in a CNN, from input to final classification.\n","permalink":"https://adilsarsenov.dev/posts/convolutional-neural-networks/","summary":"Convolutional Neural Networks (CNNs) are a class of deep learning models designed to process visual data. CNNs mimic the way the human brain processes visual information, making them incredibly powerful for visual tasks.\nKey Concepts of CNN Components Convolutional Layer The convolutional layer applies filter(kernel) to the input image to extract features like edges and textures.\nConvolutional Layer: The primary building block of a CNN, responsible for feature extraction. Filter (Kernel): A small matrix that slides over the input image, performing multiplications and summations to produce a feature map.","title":"Convolutional Neural Networks - CNNs"}]